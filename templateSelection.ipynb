{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shivam_templateSelection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op5FbCwQUoXz",
        "outputId": "e5d4eae0-2c55-470b-8b39-e6d7380417a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCedB6ShYipK",
        "outputId": "1ee28ea5-2aea-4086-aee2-73a2b1799584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTUW7kO7LzSz"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "device = None\n",
        "\n",
        "class BertClassification(torch.nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super().__init__()\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\n",
        "            \"bert-base-uncased\",\n",
        "            num_labels=num_labels,\n",
        "            output_attentions=False,\n",
        "            output_hidden_states=False\n",
        "        )\n",
        "\n",
        "    def flat_accuracy(self, preds, labels):\n",
        "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "        labels_flat = labels.flatten()\n",
        "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "    def format_time(self, elapsed):\n",
        "        '''\n",
        "        Takes a time in seconds and returns a string hh:mm:ss\n",
        "        '''\n",
        "        # Round to the nearest second.\n",
        "        elapsed_rounded = int(round((elapsed)))\n",
        "        # Format as hh:mm:ss\n",
        "        return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "    def train(self, train_dataloader, validation_dataloader, epochs=1):\n",
        "        self.model.cuda()\n",
        "        optimizer = AdamW(self.model.parameters(),\n",
        "                          lr=7e-5,  # args.learning_rate - default is 5e-5\n",
        "                          eps=2e-8  # args.adam_epsilon  - default is 1e-8.\n",
        "                          )\n",
        "        total_steps = len(train_dataloader) * epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                    num_warmup_steps=0,\n",
        "                                                    num_training_steps=total_steps)\n",
        "\n",
        "        seed_val = 66\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "        self.training_stats = []\n",
        "\n",
        "        total_t0 = time.time()\n",
        "\n",
        "        for epoch_i in range(0, epochs):\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            # Measure how long the training epoch takes.\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Reset the total loss for this epoch.\n",
        "            total_train_loss = 0\n",
        "\n",
        "            self.model.train()\n",
        "\n",
        "            # For each batch of training data...\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "                # Progress update every 40 batches.\n",
        "                if step % 40 == 0 and not step == 0:\n",
        "                    # Calculate elapsed time in minutes.\n",
        "                    elapsed = self.format_time(time.time() - t0)\n",
        "\n",
        "                    # Report progress.\n",
        "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "\n",
        "                self.model.zero_grad()\n",
        "\n",
        "                loss, logits = self.model(b_input_ids,\n",
        "                                          token_type_ids=None,\n",
        "                                          attention_mask=b_input_mask,\n",
        "                                          labels=b_labels)\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate the gradients.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0.\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "            # Measure how long this epoch took.\n",
        "            training_time = self.format_time(time.time() - t0)\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "            print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "            # Validation\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Put the model in evaluation mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Tracking variables\n",
        "            total_eval_accuracy = 0\n",
        "            total_eval_loss = 0\n",
        "            nb_eval_steps = 0\n",
        "\n",
        "            # Evaluate data for one epoch\n",
        "            for batch in validation_dataloader:\n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "\n",
        "                # Tell pytorch not to bother with constructing the compute graph during\n",
        "                # the forward pass, since this is only needed for backprop (training).\n",
        "                with torch.no_grad():\n",
        "                    (loss, logits) = self.model(b_input_ids,\n",
        "                                                token_type_ids=None,\n",
        "                                                attention_mask=b_input_mask,\n",
        "                                                labels=b_labels)\n",
        "\n",
        "                # Accumulate the validation loss.\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "                # Calculate the accuracy for this batch of test sentences, and\n",
        "                # accumulate it over all batches.\n",
        "                total_eval_accuracy += self.flat_accuracy(logits, label_ids)\n",
        "\n",
        "            avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "            print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "            avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "            validation_time = self.format_time(time.time() - t0)\n",
        "\n",
        "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "            print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "            # Record all statistics from this epoch.\n",
        "            self.training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                    'Valid. Accur.': avg_val_accuracy,\n",
        "                    'Training Time': training_time,\n",
        "                    'Validation Time': validation_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "        print(\"Total training took {:} (h:mm:ss)\".format(self.format_time(time.time() - total_t0)))\n",
        "\n",
        "    def predict(self, validation_dataloader):\n",
        "        print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "        # Put model in evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Tracking variables\n",
        "        self.predictions = []\n",
        "\n",
        "        # Predict\n",
        "        for batch in validation_dataloader:\n",
        "            # Add batch to GPU\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            b_input_ids, b_input_mask, _ = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(b_input_ids, token_type_ids=None,\n",
        "                                     attention_mask=b_input_mask)\n",
        "\n",
        "            logits = outputs[0]\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            # label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            # Store predictions and true labels\n",
        "            self.predictions.append(logits)\n",
        "\n",
        "        print('    DONE.')\n",
        "\n",
        "    def single_predict(self, input_ids, input_mask):\n",
        "        self.model.eval()\n",
        "        outputs = self.model(input_ids, token_type_ids=None,\n",
        "                             attention_mask=input_mask)\n",
        "        logits = outputs[0]\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        return logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yWK9QJLheWf"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "\n",
        "device = None\n",
        "templates_map_reverse = {0: 'high/drunk guy',\n",
        "                         1: 'Angry Cat Meme',\n",
        "                         2: 'Desk Flip Rage Guy',\n",
        "                         3: 'Willy Wonka',\n",
        "                         4: 'african children dancing',\n",
        "                         5: 'Annoying Gamer Kid',\n",
        "                         6: 'burning house girl',\n",
        "                         7: 'evil plan kid',\n",
        "                         8: 'You shall not pass',\n",
        "                         9: 'Black Kid',\n",
        "                         10: 'Grumpy Cat Santa Hat',\n",
        "                         11: 'FU*CK THAT GUY',\n",
        "                         12: 'so doge',\n",
        "                         13: 'Success Kid',\n",
        "                         14: 'Joseph Ducreux',\n",
        "                         15: 'Skeptical african kid',\n",
        "                         16: 'Anchorman Birthday',\n",
        "                         17: 'Awkward Seal',\n",
        "                         18: 'Grumpy Cat 2',\n",
        "                         19: 'Y U No'}\n",
        "\n",
        "def getMemeTemplate(input, modelFilePath):\n",
        "    print('Loading BERT tokenizer...')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "    loaded_model = BertClassification(25)\n",
        "    loaded_model.load_state_dict(torch.load(modelFilePath))\n",
        "    loaded_model.model.eval()\n",
        "    prediction_label = []\n",
        "    for in_sent in input:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            in_sent,  # Sentence to encode.\n",
        "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "            max_length=50,  # Pad & truncate all sentences.\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,  # Construct attn. masks.\n",
        "            return_tensors='pt',  # Return pytorch tensors.\n",
        "        )\n",
        "        input_ids = torch.cat([encoded_dict['input_ids']], dim=0)\n",
        "        attention_masks = torch.cat([encoded_dict['attention_mask']], dim=0)\n",
        "        outputs = loaded_model.model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        prediction_label.append(templates_map_reverse[logits.argmax(axis=1)[0]])\n",
        "    return prediction_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrfhb8oeNpAO",
        "outputId": "2f5ddda9-5ac0-48b8-f2bb-c318458e8882",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "filename = '/content/drive/My Drive/Resume/meme_template_selector.pt'\n",
        "input = [\"Eating dinner with a friends family\", \"a dog is playing with a toy toy\",\n",
        "         \"a cople of women walking down a street\", \"a man riding a wave on top of a surf board\",\n",
        "         \"I am not angry, I am happiness challenged\", \"I purred once, it was awful\",\n",
        "         \"this one again? you must be new here\", \"Say again, I wasn't listening\"]\n",
        "print(getMemeTemplate(input, filename))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['Awkward Seal', 'Awkward Seal', 'Awkward Seal', 'Awkward Seal', 'Grumpy Cat 2', 'Grumpy Cat 2', 'Willy Wonka', 'Angry Cat Meme']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
