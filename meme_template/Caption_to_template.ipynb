{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Caption_to_template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f706f4b0b3d3446391698b32361db480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fe855b72424245bfb050a1358ead88c8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1c34b4b4126944b1a8e6fbdde380c691",
              "IPY_MODEL_a74e7504c099486e8270e8fd89e6c873"
            ]
          }
        },
        "fe855b72424245bfb050a1358ead88c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c34b4b4126944b1a8e6fbdde380c691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_36fcb5520fb04ee0a348d23249632c24",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad6a74d64d714d398497771bb159a736"
          }
        },
        "a74e7504c099486e8270e8fd89e6c873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a760b4a36f8f47efadb1bb9adb989046",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/3 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b3ceca311caf4d599e60f33194495763"
          }
        },
        "36fcb5520fb04ee0a348d23249632c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad6a74d64d714d398497771bb159a736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a760b4a36f8f47efadb1bb9adb989046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b3ceca311caf4d599e60f33194495763": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "syXqOBRAwASC",
        "outputId": "3636a7d9-d0f5-4451-a5c8-d1ac9cb641fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Poh7Q92AwDRY",
        "outputId": "20d65669-59fa-4ec5-ab4b-54de8728d647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "!pip install pytorch-transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.1.94)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.16.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.6.20)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.13 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.19.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.13->boto3->pytorch-transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLXKxpXJSuZN",
        "outputId": "a989138f-ed4a-4077-bf0b-fb9572593161",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@markdown Utility function (run on each Runtime restart)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def download_from_gdrive(gdrive_id, filename):\n",
        "    !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$gdrive_id -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"$gdrive_id -O $filename && rm -rf /tmp/cookies.txt\n",
        "\n",
        "import torch\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('DEVICE:', DEVICE)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fvoBiv1S0Te"
      },
      "source": [
        "DATA_DIR = 'memes900k'\n",
        "CAPTIONS_FILE = os.path.join(DATA_DIR, 'captions.txt')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1C7z_lbTRyO"
      },
      "source": [
        "#@markdown Load and process the file with data and checkpoints Google Drive IDs\n",
        "\n",
        "GDRIVE_ID = '1S4QwcuznRxLlxkIT0Lb6vIuqDTib41B3'\n",
        "FILE_IDS_NAME = 'file_ids.txt'\n",
        "\n",
        "download_from_gdrive(GDRIVE_ID, FILE_IDS_NAME)\n",
        "\n",
        "FILE_IDS = {}\n",
        "with open(FILE_IDS_NAME, 'r') as f:\n",
        "    for line in f:\n",
        "        name, gid = line.strip().split('\\t')\n",
        "        FILE_IDS[name] = gid\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geTsncqzSzX5"
      },
      "source": [
        "#@title Load dataset\n",
        "\n",
        "# full dataset\n",
        "print('Loading the dataset from Google Drive')\n",
        "fname = f'{DATA_DIR}.zip'\n",
        "download_from_gdrive(FILE_IDS[fname], fname)\n",
        "!unzip -o {DATA_DIR}\n",
        "clear_output()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA4eENI5X83Q"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ats0G4-bxCOf"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "import random\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm_notebook, trange\n",
        "\n",
        "\n",
        "from pytorch_transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer,\n",
        "                                  XLMConfig, XLMForSequenceClassification, XLMTokenizer, \n",
        "                                  XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer,\n",
        "                                  RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "\n",
        "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
        "\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ3B5sy9YhBC"
      },
      "source": [
        "args = {\n",
        "    'data_dir': 'memes900k/',\n",
        "    'model_type':  'roberta',\n",
        "    'model_name': 'roberta-base',\n",
        "    'task_name': 'binary',\n",
        "    'output_dir': 'outputs/',\n",
        "    'cache_dir': 'cache/',\n",
        "    'do_train': True,\n",
        "    'do_eval': True,\n",
        "    'fp16': False,\n",
        "    'fp16_opt_level': 'O1',\n",
        "    'max_seq_length': 128,\n",
        "    'output_mode': 'classification',\n",
        "    'train_batch_size': 8,\n",
        "    'eval_batch_size': 8,\n",
        "\n",
        "    'gradient_accumulation_steps': 1,\n",
        "    'num_train_epochs': 1,\n",
        "    'weight_decay': 0,\n",
        "    'learning_rate': 4e-5,\n",
        "    'adam_epsilon': 1e-8,\n",
        "    'warmup_steps': 0,\n",
        "    'max_grad_norm': 1.0,\n",
        "\n",
        "    'logging_steps': 50,\n",
        "    'evaluate_during_training': False,\n",
        "    'save_steps': 2000,\n",
        "    'eval_all_checkpoints': True,\n",
        "\n",
        "    'overwrite_output_dir': False,\n",
        "    'reprocess_input_data': False,\n",
        "    \n",
        "}\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNf2MbaKYnwh"
      },
      "source": [
        "with open('args.json', 'w') as f:\n",
        "    json.dump(args, f)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY0lrcgnYqT5"
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
        "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsVCIcZSYvbC",
        "outputId": "c691da07-0ee6-49cc-ad4a-5d1cac4df412",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "config = config_class.from_pretrained(args['model_name'], num_labels=300)\n",
        "tokenizer = tokenizer_class.from_pretrained(args['model_name'],num_labels=300)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/pytorch_transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "INFO:pytorch_transformers.modeling_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 300,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/pytorch_transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/pytorch_transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEZZUgHwYzRg",
        "outputId": "cb5a0f91-24f0-48c4-a33c-188cff120734",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = model_class.from_pretrained(args['model_name'],num_labels=300)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/pytorch_transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "INFO:pytorch_transformers.modeling_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 300,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "INFO:pytorch_transformers.modeling_utils:Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "INFO:pytorch_transformers.modeling_utils:Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKWuWAktbw2D"
      },
      "source": [
        "def prepare_features(seq_1, max_seq_length = 20, \n",
        "             zero_pad = True, include_CLS_token = True, include_SEP_token = True):\n",
        "    ## Tokenzine Input\n",
        "    tokens_a = tokenizer.tokenize(seq_1)\n",
        "   \n",
        "    ## Truncate\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "    ## Initialize Tokens\n",
        "    tokens = []\n",
        "    if include_CLS_token:\n",
        "        tokens.append(tokenizer.cls_token)\n",
        "    ## Add Tokens and separators\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "\n",
        "    if include_SEP_token:\n",
        "        tokens.append(tokenizer.sep_token)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    ## Input Mask \n",
        "    input_mask = [1] * len(input_ids)\n",
        "    ## Zero-pad sequence lenght\n",
        "    if zero_pad:\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "  \n",
        "    return torch.tensor(input_ids).unsqueeze(0), input_mask"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DimPqkgdf6g",
        "outputId": "3ebcd0de-c680-45d1-cd43-1e1177a2ae0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "msg = \"My dog is cute!\"\n",
        "print(prepare_features(msg)[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[    0,  1308,  2335,    16, 11962,   328,     2,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAbp3jFW0L5t"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j77a7anJ13MZ"
      },
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "# select from github dataset\n",
        "with open(\"memes900k/captions.txt\") as file:\n",
        "  for line in file.readlines():\n",
        "    l,_,text = line.split('\\t')\n",
        "    y_train.append(l.strip())\n",
        "    text = text.strip().replace('<sep>',\"[SEP]\")\n",
        "    x_train.append(text)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EmrQPUX8XUQ",
        "outputId": "2b1bfdcd-194c-45d7-af08-432e7a76eca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "templates_map = dict(zip(set(y_train),range(300)))\n",
        "templates_map_reverse = {v:k for k,v in templates_map.items()}\n",
        "len(templates_map)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "167b3t61GlgC"
      },
      "source": [
        "y_train = [templates_map[i] for i in y_train if i ]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be6ZRy6iVrTS"
      },
      "source": [
        "x_test = []\n",
        "y_test = []\n",
        "with open(\"memes900k/captions_test.txt\") as file:\n",
        "  for line in file.readlines():\n",
        "    l,_,text = line.split('\\t')\n",
        "    if l in templates_map:\n",
        "      y_test.append(templates_map[l])\n",
        "      text = text.strip().replace('<sep>',\"[SEP]\")\n",
        "      x_test.append(text)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihsrxjr9SGRe",
        "outputId": "1c617e6f-afee-4fa4-eb3a-21e85bf6df87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(set(y_train))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iIAaw7I0HXN",
        "outputId": "4f492da5-2914-4647-9e77-8714696a13ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in x_train:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    \n",
        "    encoded_dict,attention = prepare_features(sent)\n",
        "    \n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict)\n",
        "    attention_masks.append(torch.Tensor(attention))\n",
        "    \n",
        "\n",
        "# Convert the lists into tensors.\n",
        "\n",
        "\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', x_train[3])\n",
        "print('Token IDs:', input_ids[3])\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  TED [SEP] y u no tell us how you met their mother\n",
            "Token IDs: tensor([[    0, 32690,   646,  3388,   510,   742,  1423,  1717,   117,  1137,\n",
            "           201,   141,    47,  1145,    49,   985,     2,     0,     0,     0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjqmTQbltq6C"
      },
      "source": [
        "# Convert the lists into tensors.\n",
        "\n",
        "input_ids_t = torch.cat(input_ids, dim=0)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGse8PXkw_kG"
      },
      "source": [
        "attention_masks_t = torch.stack(attention_masks, dim=0)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eupbwl3iC9hx"
      },
      "source": [
        "from torch.utils.data import DataLoader, random_split,WeightedRandomSampler, TensorDataset\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_train,input_val,label_train,label_val= train_test_split(input_ids_t,torch.tensor(y_train), test_size=0.01)\n",
        "\n",
        "train_dataset = TensorDataset(input_train, label_train)\n",
        "val_dataset = TensorDataset(input_val, label_val)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM0SsPztEwD_"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 64\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = 120 # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-vopcfvd4yy"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC9RIia0aTWb"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-03\n",
        "optimizer = optim.Adam(params =  model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5XOUSplVcxX",
        "outputId": "127c58f6-0d9b-430b-aea8-230de12f176e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f706f4b0b3d3446391698b32361db480",
            "fe855b72424245bfb050a1358ead88c8",
            "1c34b4b4126944b1a8e6fbdde380c691",
            "a74e7504c099486e8270e8fd89e6c873",
            "36fcb5520fb04ee0a348d23249632c24",
            "ad6a74d64d714d398497771bb159a736",
            "a760b4a36f8f47efadb1bb9adb989046",
            "b3ceca311caf4d599e60f33194495763"
          ]
        }
      },
      "source": [
        "max_epochs = 3\n",
        "model = model.train()\n",
        "for epoch in tqdm_notebook(range(max_epochs)):\n",
        "    print(\"EPOCH -- {}\".format(epoch))\n",
        "    for i, j in enumerate(train_dataloader):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        sent=j[0]\n",
        "        label=j[1]\n",
        "      \n",
        "        sent = sent.squeeze(0)\n",
        "        if torch.cuda.is_available():\n",
        "          sent = sent\n",
        "          label = label\n",
        "\n",
        "        output = model.forward(sent)[0]\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        \n",
        "        loss = loss_function(output, label)\n",
        "        #print(\"training\",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if i%100 == 0:\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for k in validation_dataloader:\n",
        "                sent=k[0]\n",
        "                label=k[1]\n",
        "                sent = sent.squeeze(0)\n",
        "                if torch.cuda.is_available():\n",
        "                  sent = sent\n",
        "                  label = label\n",
        "                output = model.forward(sent)[0]\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total += label.size(0)\n",
        "                correct += (predicted.cpu() == label.cpu()).sum()\n",
        "                #print(\"validation\")\n",
        "            accuracy = 100.00 * correct.numpy() / total\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}%'.format(i, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f706f4b0b3d3446391698b32361db480",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH -- 0\n",
            "training tensor(5.7090, grad_fn=<NllLossBackward>)\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "validation\n",
            "Iteration: 0. Loss: 5.709024906158447. Accuracy: 0.24444444444444444%\n",
            "training tensor(5.7152, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7005, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7136, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7398, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7007, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6999, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7340, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7104, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7068, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6874, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6874, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7481, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7202, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7032, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7038, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6972, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7026, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7263, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6995, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7380, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7331, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7089, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7668, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6740, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6952, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7299, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7291, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7319, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7021, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7364, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7098, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6986, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6883, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7541, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7010, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7514, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7263, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6915, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7420, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7276, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6956, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7150, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7186, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7386, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6854, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7058, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6631, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7106, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7176, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7196, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7259, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7259, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7080, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7083, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6857, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7343, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6895, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7113, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7096, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7394, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7255, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7395, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6910, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7181, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7295, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7527, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7185, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6997, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7097, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6907, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7258, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7393, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7131, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7194, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7176, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6842, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6780, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7138, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.6968, grad_fn=<NllLossBackward>)\n",
            "training tensor(5.7246, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz28JmTlFxco"
      },
      "source": [
        "torch.save(model.state_dict(), 'roberat.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIcELLw6Fbpi",
        "outputId": "2f844d91-33ce-4d64-84c5-89718bc22515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=bc.training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.09</td>\n",
              "      <td>2.70</td>\n",
              "      <td>0.40</td>\n",
              "      <td>1:44:41</td>\n",
              "      <td>0:03:42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.49</td>\n",
              "      <td>2.56</td>\n",
              "      <td>0.42</td>\n",
              "      <td>1:44:56</td>\n",
              "      <td>0:03:42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.11</td>\n",
              "      <td>2.54</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1:44:43</td>\n",
              "      <td>0:03:42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.74</td>\n",
              "      <td>2.59</td>\n",
              "      <td>0.44</td>\n",
              "      <td>1:44:46</td>\n",
              "      <td>0:03:42</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               3.09         2.70           0.40       1:44:41         0:03:42\n",
              "2               2.49         2.56           0.42       1:44:56         0:03:42\n",
              "3               2.11         2.54           0.43       1:44:43         0:03:42\n",
              "4               1.74         2.59           0.44       1:44:46         0:03:42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhZm83Bqqpyf"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hej8I6hkHVCi"
      },
      "source": [
        "x_test1 = [\"a group of people sitting on a bench\",\"a dog is playing with a toy toy\",\"a cople of women walking down a street\",\"a man riding a wave on top of a surf board\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ns9Kj9Gqruh",
        "outputId": "87ae3dec-ae0a-47ca-9041-e6a83df45ea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_reply(msg):\n",
        "  model.eval()\n",
        "  input_msg, _ = prepare_features(msg)\n",
        "  if torch.cuda.is_available():\n",
        "    input_msg = input_msg\n",
        "  output = model(input_msg)[0]\n",
        "  _, pred_label = torch.max(output.data, 1)\n",
        "  prediction=list(templates_map_reverse.keys())[pred_label]\n",
        "  return prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-1ke9DQGRuS"
      },
      "source": [
        "for i in x_test1:\n",
        "  print(get_reply(i))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}