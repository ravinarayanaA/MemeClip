<!DOCTYPE html>
<html>
<head>
    <title>ClipMe: Meme Clip Generation</title>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="LaTeXML.css" type="text/css">
    <link rel="stylesheet" href="ltx-article.css" type="text/css">
</head>
<body>
<div class="ltx_page_main">
    <div class="ltx_page_content">
        <article class="ltx_document ltx_authors_1line">
            <h1 class="ltx_title ltx_title_document">ClipMe: Meme Clip Generation</h1>
            <div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mehak Piplani
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter">mpiplani@usc.edu</span>
<br class="ltx_break"> Shivam
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter">shivam@usc.edu</span>

 Rishabh Bansal
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter">rishabhb@usc.edu</span>

 Che-Pai Kung
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter">chepaiku@usc.edu</span>

 Ravinarayana Adkathimar
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter">adkathim@usc.edu</span>


</span></span>
            </div>

            <div class="ltx_abstract">
                <h6 class="ltx_title ltx_title_abstract">Abstract</h6>

                <p class="ltx_p">Inspired by the idea of internet image memes, this work introduces a novel automated
                    meme clip generation system. The system follows a probabilistic approach to generate a meme clip for
                    any given image using the ClipMe architecture. For a given image, a caption is extracted by a
                    attention-based deep-layer LSTM model that aids in the generation of a meme by combining a meme
                    template image and a meme caption employing an encoder-decoder model. This meme is mapped to
                    suitable audio to form the meme clip. The quality of the generated meme clip is assessed through
                    both automated and human evaluations.</p>

            </div>
            <section id="S1" class="ltx_section">
                <h2 class="ltx_title ltx_title_section">
                    <span class="ltx_tag ltx_tag_section">1 </span>Motivation</h2>

                <div id="S1.p1" class="ltx_para">
                    <p class="ltx_p">Internet image memes have become widely used by people on social media. Memes are
                        often viewed as a form of entertainment. A good meme can also be useful in advertisements, which
                        may make the content noticed and increase the click-through rate. Besides, funny audio is easy
                        to catch people’s attention, and we believe that it may bring more joy to the users. Therefore,
                        we introduce an idea to implement a deep neural network model to automatically generate
                        high-quality memes clips from images incorporating a meme and humorous audio to grab attention
                        and bring laughter to society.
                    </p>
                </div>
            </section>
            <section id="S2" class="ltx_section">
                <h2 class="ltx_title ltx_title_section">
                    <span class="ltx_tag ltx_tag_section">2 </span>Contributions</h2>

                <div id="S2.p1" class="ltx_para">
                    <p class="ltx_p">The state of the art methods focus on creating a humorous caption for the given
                        meme template or generation of memes based on textual input.
                        We propose a model architecture named ClipMe that generates a meme clip (meme with audio) from a
                        random input image. Our network will first extract an image description for the input image and
                        leverage a template selection network to select the best-matched meme template for the input
                        image. Then we will use this image description and the selected template to get a funny meme
                        caption and map the generated meme caption to the closest match funny audio.</p>
                </div>
                <div id="S2.p2" class="ltx_para">
                    <p class="ltx_p">To summarize our main contributions of our work include:</p>
                    <ol id="S2.I1" class="ltx_enumerate">
                        <li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
                            <span class="ltx_tag ltx_tag_item">1.</span>
                            <div id="S2.I1.i1.p1" class="ltx_para">
                                <p class="ltx_p"><span class="ltx_text ltx_font_bold">ClipMe:</span> Fusion of meme
                                    template image, a humorous caption, and a suitable audio to create meme clips.</p>
                            </div>
                        </li>
                        <li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
                            <span class="ltx_tag ltx_tag_item">2.</span>
                            <div id="S2.I1.i2.p1" class="ltx_para">
                                <p class="ltx_p"><span
                                        class="ltx_text ltx_font_bold">Creation of customized dataset</span>: Our
                                    network is comprised of four modules where each has a different set of inputs and
                                    outputs. To fine-tune our network, we propose to create a custom hand-engineered
                                    dataset that includes
                                    input image with image-caption mappings, caption-template mappings, template+caption
                                    to meme mappings and meme to audio mappings.</p>
                            </div>
                        </li>
                    </ol>
                </div>
            </section>
            <section id="S3" class="ltx_section">
                <h2 class="ltx_title ltx_title_section">
                    <span class="ltx_tag ltx_tag_section">3 </span>Related Work</h2>

                <div id="S3.p1" class="ltx_para">
                    <p class="ltx_p">Automatic meme generation is a recent research field, and only a few authors have
                        worked in this field. The existing approaches treat the meme generation as a caption selection
                        or caption generation problem. The authors <cite class="ltx_cite ltx_citemacro_cite">Wang and
                            Wen (<a href="#bib.bib8"
                                    title="I can has cheezburger? a nonparanormal approach to combining textual and visual information for predicting and generating popular meme descriptions"
                                    class="ltx_ref">2015</a>)</cite> combined an image and its text description to
                        select a meme caption from a corpus-based on a ranking algorithm. The study by <cite
                                class="ltx_cite ltx_citemacro_cite">AbelL.Peirson and Tolunay (<a href="#bib.bib9"
                                                                                                  title="Dank learning: generating memes using deep neural networks"
                                                                                                  class="ltx_ref">2018</a>)</cite>
                        focused on extending Natural Language Description Generation to generating a meme caption using
                        an encoder-decoder model with attention (<cite class="ltx_cite ltx_citemacro_cite">Luong<span
                                class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib10"
                                                                                 title="Effective approaches to attention-based neural machine translation"
                                                                                 class="ltx_ref">2015</a>)</cite>)
                        mechanism. The study, <cite class="ltx_cite ltx_citemacro_cite">Sadasivam<span
                                class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib11"
                                                                                 title="MemeBot: towards automatic image meme generation"
                                                                                 class="ltx_ref">2020</a>)</cite>
                        implements a transformer based encoder-decoder model to generate a meme caption conditioned on
                        both the input sentence and the selected meme template.</p>
                </div>
            </section>
            <section id="S4" class="ltx_section">
                <h2 class="ltx_title ltx_title_section">
                    <span class="ltx_tag ltx_tag_section">4 </span>Problem Formulation</h2>

                <figure id="S4.F1" class="ltx_figure">
                    <p class="ltx_p ltx_align_center"><a
                            href="https://drive.google.com/file/d/1mX1kQNPaGG57DV2P_HFsrT9OAKy6njAq/view?usp=sharing"
                            title="" class="ltx_ref ltx_href"><img src="model.png" id="S4.F1.g1" class="ltx_graphics"
                                                                   width="355" height="236" alt=""></a></p>
                    <figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Meme
                        Clip Generation via ClipMe (click me)
                    </figcaption>
                </figure>
                <div id="S4.p1" class="ltx_para">
                    <p class="ltx_p">The input of the model as described in Fig <a href="#S4.F1"
                                                                                   title="Figure 1 ‣ 4 Problem Formulation ‣ ClipMe: Meme Clip Generation"
                                                                                   class="ltx_ref"><span
                            class="ltx_text ltx_ref_tag">1</span></a> is a single raw image, and the caption generator
                        module outputs a caption y encoded as a sequence of
                        <math id="S4.p1.m1" class="ltx_Math" alttext="1-of-K" display="inline">
                            <mrow>
                                <mn>1</mn>
                                <mo>-</mo>
                                <mrow>
                                    <mi>o</mi>
                                    <mo>⁢</mo>
                                    <mi>f</mi>
                                </mrow>
                                <mo>-</mo>
                                <mi>K</mi>
                            </mrow>
                        </math>
                        encoded
                        words.
                    </p>
                    <table id="S4.Ex1" class="ltx_equation ltx_eqn_table">

                        <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
                            <td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
                            <td class="ltx_eqn_cell ltx_align_center">
                                <math id="S4.Ex1.m1" class="ltx_Math"
                                      alttext="y=\{y_{1},...,y_{C}\}~{}~{}~{}y_{i}\in R^{k}" display="block">
                                    <mrow>
                                        <mi>y</mi>
                                        <mo>=</mo>
                                        <mrow>
                                            <mrow>
                                                <mo stretchy="false">{</mo>
                                                <msub>
                                                    <mi>y</mi>
                                                    <mn>1</mn>
                                                </msub>
                                                <mo>,</mo>
                                                <mi mathvariant="normal">…</mi>
                                                <mo>,</mo>
                                                <msub>
                                                    <mi>y</mi>
                                                    <mi>C</mi>
                                                </msub>
                                                <mo rspace="12.4pt" stretchy="false">}</mo>
                                            </mrow>
                                            <mo>⁢</mo>
                                            <msub>
                                                <mi>y</mi>
                                                <mi>i</mi>
                                            </msub>
                                        </mrow>
                                        <mo>∈</mo>
                                        <msup>
                                            <mi>R</mi>
                                            <mi>k</mi>
                                        </msup>
                                    </mrow>
                                </math>
                            </td>
                            <td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
                        </tr>
                    </table>
                    <p class="ltx_p">Where
                        <math id="S4.p1.m2" class="ltx_Math" alttext="K" display="inline">
                            <mi>K</mi>
                        </math>
                        is the size of the vocabulary, and
                        <math id="S4.p1.m3" class="ltx_Math" alttext="C" display="inline">
                            <mi>C</mi>
                        </math>
                        is the length of the caption.
                        The caption
                        <math id="S4.p1.m4" class="ltx_Math" alttext="y" display="inline">
                            <mi>y</mi>
                        </math>
                        is mapped to a meme template encoding
                        <math id="S4.p1.m5" class="ltx_Math" alttext="T" display="inline">
                            <mi>T</mi>
                        </math>
                        using Bert sequence classification. This caption
                        <math id="S4.p1.m6" class="ltx_Math" alttext="y" display="inline">
                            <mi>y</mi>
                        </math>
                        and a meme template encoding
                        <math id="S4.p1.m7" class="ltx_Math" alttext="T" display="inline">
                            <mi>T</mi>
                        </math>
                        is passed through an encoder-decoder module to map the input to a humorous caption
                        <math id="S4.p1.m8" class="ltx_Math" alttext="z" display="inline">
                            <mi>z</mi>
                        </math>
                        .
                    </p>
                    <table id="S4.Ex2" class="ltx_equation ltx_eqn_table">

                        <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
                            <td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
                            <td class="ltx_eqn_cell ltx_align_center">
                                <math id="S4.Ex2.m1" class="ltx_Math" alttext="f:y,T\mapsto z" display="block">
                                    <mrow>
                                        <mi>f</mi>
                                        <mo>:</mo>
                                        <mrow>
                                            <mrow>
                                                <mi>y</mi>
                                                <mo>,</mo>
                                                <mi>T</mi>
                                            </mrow>
                                            <mo>↦</mo>
                                            <mi>z</mi>
                                        </mrow>
                                    </mrow>
                                </math>
                            </td>
                            <td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
                        </tr>
                    </table>
                    <br class="ltx_break">
                    <p class="ltx_p">The meme caption
                        <math id="S4.p1.m9" class="ltx_Math" alttext="z" display="inline">
                            <mi>z</mi>
                        </math>
                        is passed to audio mapping segment predicting an audio signal
                        <math id="S4.p1.m10" class="ltx_Math" alttext="a" display="inline">
                            <mi>a</mi>
                        </math>
                        to produce a meme clip combining meme template
                        <math id="S4.p1.m11" class="ltx_Math" alttext="T" display="inline">
                            <mi>T</mi>
                        </math>
                        , caption
                        <math id="S4.p1.m12" class="ltx_Math" alttext="z" display="inline">
                            <mi>z</mi>
                        </math>
                        , and audio
                        <math id="S4.p1.m13" class="ltx_Math" alttext="a" display="inline">
                            <mi>a</mi>
                        </math>
                        .
                    </p>
                </div>
            </section>
            <section id="S5" class="ltx_section">
                <h2 class="ltx_title ltx_title_section">
                    <span class="ltx_tag ltx_tag_section">5 </span>Dataset</h2>

                <div id="S5.p1" class="ltx_para">
                    <p class="ltx_p">For the image captioning module of ClipMe, the Microsoft COCO dataset will be
                        utilized. The dataset contains easily recognizable photos of 91 objects types with a total of
                        2.5 million labeled instances in 328k images.
                        <br class="ltx_break">To implement and validate the meme caption generator and meme template
                        selection module, we crawl and preprocess a large-scale meme dataset consisting of 900,000 meme
                        captions for 300 meme template images collected from <a href="https://memegenerator.net/"
                                                                                title="" class="ltx_ref ltx_href">MemeGenerator</a>
                        website.
                        <br class="ltx_break">For adding humorous audios to the meme clip, two websites were crawled
                        namely, <a href="https://memebot.life/" title="" class="ltx_ref ltx_href">memebot.life</a> and
                        the <a href="https://www.voicy.network/" title="" class="ltx_ref ltx_href">voicy network</a>. A
                        collection of 1147 audio files has been built incorporating diverse labels.
                    </p>
                </div>
            </section>
            <section id="S6" class="ltx_section">
                <h2 class="ltx_title ltx_title_section">
                    <span class="ltx_tag ltx_tag_section">6 </span>Our Approach</h2>

                <div id="S6.p1" class="ltx_para">
                    <p class="ltx_p">A novel architecture is designed to generate meme clips, <span
                            class="ltx_text ltx_font_bold">ClipMe</span> comprising of four modules: Image Caption
                        Generation, Meme Template Selection, Meme Generation, and Audio Mapper.</p>
                </div>
                <section id="S6.SS1" class="ltx_subsection">
                    <h3 class="ltx_title ltx_title_subsection">
                        <span class="ltx_tag ltx_tag_subsection">6.1 </span>Image Caption Generation</h3>

                    <div id="S6.SS1.p1" class="ltx_para">
                        <p class="ltx_p">There are three components in our image caption generation model:</p>
                        <ul id="S6.I1" class="ltx_itemize">
                            <li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
                                <span class="ltx_tag ltx_tag_item">•</span>
                                <div id="S6.I1.i1.p1" class="ltx_para">
                                    <p class="ltx_p"><span class="ltx_text ltx_font_bold">Encoder-Decoder
<br class="ltx_break"></span>The encoder takes a colored image as input and converts it into a summarized representation
                                        of the original image. We are using 101 layered Residual Network (<cite
                                                class="ltx_cite ltx_citemacro_cite">He (<a href="#bib.bib15"
                                                                                           title="Deep residual learning for image recognition."
                                                                                           class="ltx_ref">2015</a>)</cite>)
                                        trained on the ImageNet (<cite class="ltx_cite ltx_citemacro_cite">Lin (<a
                                                href="#bib.bib14" title="Microsoft coco: common objects in context."
                                                class="ltx_ref">2014</a>)</cite>) classification task for decoding an
                                        image. Architectures of the encoder are shown in <a href="#S6.F2"
                                                                                            title="Figure 2 ‣ 1st item ‣ 6.1 Image Caption Generation ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                                                            class="ltx_ref"><span
                                                class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S6.F3"
                                                                                                 title="Figure 3 ‣ 1st item ‣ 6.1 Image Caption Generation ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                                                                 class="ltx_ref"><span
                                                class="ltx_text ltx_ref_tag">3</span></a>.
                                        <br class="ltx_break"></p>
                                </div>
                                <figure id="S6.F2" class="ltx_figure"><img src="Picture1.png" id="S6.F2.g1"
                                                                           class="ltx_graphics ltx_centering"
                                                                           width="355" height="113" alt="">
                                    <figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>High
                                        level architecture of encoder model. ^1
                                    </figcaption>
                                </figure>
                                <figure id="S6.F3" class="ltx_figure"><img src="Picture2.png" id="S6.F3.g1"
                                                                           class="ltx_graphics ltx_centering"
                                                                           width="284" height="267" alt="">
                                    <figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Inner
                                        architecture of encoder model with ResNet
                                    </figcaption>
                                </figure>
                                <div id="S6.I1.i1.p2" class="ltx_para">
                                    <p class="ltx_p">The decoder takes encoded images as input and generates a caption
                                        word by word. For generating the caption for the input image, we use LSTM as
                                        shown in <a href="#S6.F4"
                                                    title="Figure 4 ‣ 1st item ‣ 6.1 Image Caption Generation ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
                                </div>
                                <figure id="S6.F4" class="ltx_figure"><img src="decoder_no_att.png" id="S6.F4.g1"
                                                                           class="ltx_graphics ltx_centering"
                                                                           width="426" height="124" alt="">
                                    <figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Decoder
                                        model which generates word by word from the input image representation.
                                        <math id="S6.F4.m2" class="ltx_Math" alttext="{}^{1}" display="inline">
                                            <msup>
                                                <mi></mi>
                                                <mn>1</mn>
                                            </msup>
                                        </math>
                                    </figcaption>
                                </figure>
                            </li>
                            <li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
                                <span class="ltx_tag ltx_tag_item">•</span>
                                <div id="S6.I1.i2.p1" class="ltx_para">
                                    <p class="ltx_p"><span class="ltx_text ltx_font_bold">Attention
<br class="ltx_break"></span>To estimate the importance of a certain part of an image, we need to be aware of the
                                        sequence that is generated so far, so that we look at the image and decide what
                                        needs describing next. The attention mechanism achieves the same. Attention used
                                        in the model is soft, where the weights of the pixels add up to 1. If there are
                                        P pixels in our encoded image and
                                        <math id="S6.I1.i2.p1.m1" class="ltx_Math" alttext="\alpha_{p}"
                                              display="inline">
                                            <msub>
                                                <mi>α</mi>
                                                <mi>p</mi>
                                            </msub>
                                        </math>
                                        is weight of that pixel, then at each timestep t,
                                        <math id="S6.I1.i2.p1.m2" class="ltx_Math" alttext="\alpha_{p,t}"
                                              display="inline">
                                            <msub>
                                                <mi>α</mi>
                                                <mrow>
                                                    <mi>p</mi>
                                                    <mo>,</mo>
                                                    <mi>t</mi>
                                                </mrow>
                                            </msub>
                                        </math>
                                        follows Equation <a href="#S6.E1"
                                                            title="(1) ‣ 2nd item ‣ 6.1 Image Caption Generation ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                            class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
                                        <span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup
                                                class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span
                                                class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span
                                                class="ltx_note_type">footnotetext: </span>Reprinted from https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning.</span></span></span>
                                    </p>
                                    <table id="S6.E1" class="ltx_equation ltx_eqn_table">

                                        <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
                                            <td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
                                            <td class="ltx_eqn_cell ltx_align_center">
                                                <math id="S6.E1.m1" class="ltx_Math"
                                                      alttext="\sum\limits_{p}^{P}\alpha_{p,t}=1" display="block">
                                                    <mrow>
                                                        <mrow>
                                                            <munderover>
                                                                <mo largeop="true" movablelimits="false"
                                                                    symmetric="true">∑
                                                                </mo>
                                                                <mi>p</mi>
                                                                <mi>P</mi>
                                                            </munderover>
                                                            <msub>
                                                                <mi>α</mi>
                                                                <mrow>
                                                                    <mi>p</mi>
                                                                    <mo>,</mo>
                                                                    <mi>t</mi>
                                                                </mrow>
                                                            </msub>
                                                        </mrow>
                                                        <mo>=</mo>
                                                        <mn>1</mn>
                                                    </mrow>
                                                </math>
                                            </td>
                                            <td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
                                            <td rowspan="1"
                                                class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span
                                                    class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
                                        </tr>
                                    </table>
                                </div>
                            </li>
                            <li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
                                <span class="ltx_tag ltx_tag_item">•</span>
                                <div id="S6.I1.i3.p1" class="ltx_para">
                                    <p class="ltx_p"><span class="ltx_text ltx_font_bold">Beam Search
<br class="ltx_break"></span>Beam search uses top k candidates to generate sequences from which the sequence having
                                        highest overall score is chosen.</p>
                                </div>
                            </li>
                        </ul>
                    </div>
                    <figure id="S6.F5" class="ltx_figure"><img src="decoder_att.png" id="S6.F5.g1"
                                                               class="ltx_graphics ltx_centering" width="461"
                                                               height="268" alt="">
                        <figcaption class="ltx_caption ltx_centering"><span
                                class="ltx_tag ltx_tag_figure">Figure 5: </span>Complete architecture of the model.^1
                        </figcaption>
                    </figure>
                    <div id="S6.SS1.p2" class="ltx_para">
                        <p class="ltx_p">Figure <a href="#S6.F5"
                                                   title="Figure 5 ‣ 6.1 Image Caption Generation ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                   class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>
                            represents the complete encoder-decoder architecture with visual attention.</p>
                    </div>
                </section>
                <section id="S6.SS2" class="ltx_subsection">
                    <h3 class="ltx_title ltx_title_subsection">
                        <span class="ltx_tag ltx_tag_subsection">6.2 </span>Meme Template Selection</h3>

                    <div id="S6.SS2.p1" class="ltx_para">
                        <p class="ltx_p">Pre-trained language representation models have been popular for
                            language-related tasks. To map an input caption to a meme template, we fine-tuned models
                            like BERT, XLNet, and Roberta, followed by a linear neural network to get the template with
                            the maximum probability.
                            We also implemented the LSTM model to compare the results as mentioned in Table <a
                                    href="#S7.T2"
                                    title="Table 2 ‣ 7.1 Image Caption Generation ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
                                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. For the LSTM model,
                            the input image caption is passed through an embedding layer to extract an embedding
                            followed by the LSTM for the classification task.</p>
                    </div>
                </section>
                <section id="S6.SS3" class="ltx_subsection">
                    <h3 class="ltx_title ltx_title_subsection">
                        <span class="ltx_tag ltx_tag_subsection">6.3 </span>Meme Generation</h3>

                    <div id="S6.SS3.p1" class="ltx_para">
                        <p class="ltx_p">A meme generation module consists of the following three sub modules:</p>
                    </div>
                    <div id="S6.SS3.p2" class="ltx_para">
                        <p class="ltx_p"><span class="ltx_text ltx_font_bold">Encoder
<br class="ltx_break"></span>The motivation behind the encoder is to provide a meaningful state of the input to the
                            decoder to initiate the meme generation process. To capture the image embeddings, we utilize
                            the ResNet model trained using the ImageNet dataset. We employ an embedding layer consisting
                            of a Linear layer, BatchNorm layer, and a Dropout layer at the end. The last fully-connected
                            layer of the ResNet model is connected to the embedding layer.</p>
                    </div>
                    <figure id="S6.F6" class="ltx_figure"><img src="Model2_6.png" id="S6.F6.g1"
                                                               class="ltx_graphics ltx_centering" width="475"
                                                               height="257" alt="">
                        <figcaption class="ltx_caption ltx_centering"><span
                                class="ltx_tag ltx_tag_figure">Figure 6: </span>Captioning without Meme Template Label
                        </figcaption>
                    </figure>
                    <div id="S6.SS3.p3" class="ltx_para">
                        <p class="ltx_p">We have implemented and experimented with two different variants of the
                            proposed encoder scheme. The first variant as demonstrated in Figure <a href="#S6.F6"
                                                                                                    title="Figure 6 ‣ 6.3 Meme Generation ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                                                                    class="ltx_ref"><span
                                    class="ltx_text ltx_ref_tag">6</span></a> takes as input the meme template embedding
                            and image caption.
                            The second variant as demonstrated in Figure <a href="#S6.F7"
                                                                            title="Figure 7 ‣ 6.3 Meme Generation ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                                            class="ltx_ref"><span
                                    class="ltx_text ltx_ref_tag">7</span></a> of the encoder takes the meme template
                            label as well as meme template embedding and image caption as input.</p>
                    </div>
                    <figure id="S6.F7" class="ltx_figure"><img src="Model2.png" id="S6.F7.g1"
                                                               class="ltx_graphics ltx_centering" width="475"
                                                               height="306" alt="">
                        <figcaption class="ltx_caption ltx_centering"><span
                                class="ltx_tag ltx_tag_figure">Figure 7: </span>Captioning with Meme Template Label
                        </figcaption>
                    </figure>
                    <div id="S6.SS3.p4" class="ltx_para">
                        <ul id="S6.I2" class="ltx_itemize">
                            <li id="S6.I2.i1" class="ltx_item" style="list-style-type:none;">
                                <span class="ltx_tag ltx_tag_item">•</span>
                                <div id="S6.I2.i1.p1" class="ltx_para">
                                    <p class="ltx_p"><span class="ltx_text ltx_font_bold">Decoder
<br class="ltx_break"></span>The decoder consists of a uni-directional LSTM network. Every LSTM cell reuses the
                                        variables in the model. The output from the encoder and image caption is used to
                                        create a word embedding. These word embeddings are passed to LSTM. LSTM is run
                                        over inputs to predict the next token at each timestep. We then utilize the
                                        method of the beam search algorithm to create meaningful captions as described
                                        below.</p>
                                </div>
                            </li>
                            <li id="S6.I2.i2" class="ltx_item" style="list-style-type:none;">
                                <span class="ltx_tag ltx_tag_item">•</span>
                                <div id="S6.I2.i2.p1" class="ltx_para">
                                    <p class="ltx_p"><span class="ltx_text ltx_font_bold">Inference and Beam Search
<br class="ltx_break"></span>Meme caption generation is initiated via meme template embedding when for a single cell of
                                        the LSTM network, softmax probability distributions are computed for the words
                                        in the vocabulary. The output of an LSTM cell is fed sequentially into the next
                                        cell to generate the next word. We use beam search with a temperature
                                        hyper-parameter to filter top k scores where
                                        <math id="S6.I2.i2.p1.m1" class="ltx_Math" alttext="k" display="inline">
                                            <mi>k</mi>
                                        </math>
                                        is the beam size. We keep
                                        <math id="S6.I2.i2.p1.m2" class="ltx_Math" alttext="k" display="inline">
                                            <mi>k</mi>
                                        </math>
                                        outputs in memory at each time-step and sequentially compute their ”descendants”
                                        resulting in
                                        <math id="S6.I2.i2.p1.m3" class="ltx_Math" alttext="k" display="inline">
                                            <mi>k</mi>
                                        </math>
                                        sentences with the overall highest score. Temperature hyper-parameter helps to
                                        diverse outputs for the same template.
                                        A probability distribution
                                        <math id="S6.I2.i2.p1.m4" class="ltx_Math" alttext="p" display="inline">
                                            <mi>p</mi>
                                        </math>
                                        can be modified with temperature
                                        <math id="S6.I2.i2.p1.m5" class="ltx_Math" alttext="T" display="inline">
                                            <mi>T</mi>
                                        </math>
                                        by the function
                                        <math id="S6.I2.i2.p1.m6" class="ltx_Math" alttext="f" display="inline">
                                            <mi>f</mi>
                                        </math>
                                        as demonstrated in Equation <a href="#S6.E2"
                                                                       title="(2) ‣ 2nd item ‣ 6.3 Meme Generation ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                                       class="ltx_ref"><span
                                                class="ltx_text ltx_ref_tag">2</span></a>.
                                    </p>
                                    <table id="S6.E2" class="ltx_equation ltx_eqn_table">

                                        <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
                                            <td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
                                            <td class="ltx_eqn_cell ltx_align_center">
                                                <math id="S6.E2.m1" class="ltx_Math"
                                                      alttext="f(p)_{i}=\frac{p_{i}^{\frac{1}{T}}}{\sum_{j}p_{i}^{\frac{1}{T}}}"
                                                      display="block">
                                                    <mrow>
                                                        <mrow>
                                                            <mi>f</mi>
                                                            <mo>⁢</mo>
                                                            <msub>
                                                                <mrow>
                                                                    <mo stretchy="false">(</mo>
                                                                    <mi>p</mi>
                                                                    <mo stretchy="false">)</mo>
                                                                </mrow>
                                                                <mi>i</mi>
                                                            </msub>
                                                        </mrow>
                                                        <mo>=</mo>
                                                        <mfrac>
                                                            <msubsup>
                                                                <mi>p</mi>
                                                                <mi>i</mi>
                                                                <mfrac>
                                                                    <mn>1</mn>
                                                                    <mi>T</mi>
                                                                </mfrac>
                                                            </msubsup>
                                                            <mrow>
                                                                <msub>
                                                                    <mo largeop="true" symmetric="true">∑</mo>
                                                                    <mi>j</mi>
                                                                </msub>
                                                                <msubsup>
                                                                    <mi>p</mi>
                                                                    <mi>i</mi>
                                                                    <mfrac>
                                                                        <mn>1</mn>
                                                                        <mi>T</mi>
                                                                    </mfrac>
                                                                </msubsup>
                                                            </mrow>
                                                        </mfrac>
                                                    </mrow>
                                                </math>
                                            </td>
                                            <td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
                                            <td rowspan="1"
                                                class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span
                                                    class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
                                        </tr>
                                    </table>
                                    <p class="ltx_p">Where temperature
                                        <math id="S6.I2.i2.p1.m7" class="ltx_Math" alttext="T" display="inline">
                                            <mi>T</mi>
                                        </math>
                                        = 1 corresponds to unchanged probabilities, high
                                        <math id="S6.I2.i2.p1.m8" class="ltx_Math" alttext="T" display="inline">
                                            <mi>T</mi>
                                        </math>
                                        leads to a very flat distribution (random pick) and low
                                        <math id="S6.I2.i2.p1.m9" class="ltx_Math" alttext="T" display="inline">
                                            <mi>T</mi>
                                        </math>
                                        leads to argmax (greedy search).
                                    </p>
                                </div>
                            </li>
                        </ul>
                    </div>
                </section>
                <section id="S6.SS4" class="ltx_subsection">
                    <h3 class="ltx_title ltx_title_subsection">
                        <span class="ltx_tag ltx_tag_subsection">6.4 </span>Audio Mapper</h3>

                    <div id="S6.SS4.p1" class="ltx_para">
                        <p class="ltx_p">Semantic Similarity is the task of determining how similar two sentences are,
                            in terms of what they mean. Transformer based models allow us to produce and achieve state
                            of the art results in semantic similarity with minimal task based fine tuning, which is
                            ideal for tasks with small training data.</p>
                    </div>
                    <div id="S6.SS4.p2" class="ltx_para">
                        <p class="ltx_p">The objective behind audio mapper is to achieve accurate semantic matching
                            between the generated meme caption and the audio labels from the labeled funny audio dataset
                            to find the most suitable audio file for the meme clip. We created a textual training corpus
                            from the audio labels, and a mapping file between labels and their respective audio files.
                            To find the most semantically similar audio for a input meme caption, we fine-tuned multiple
                            transformer based pretrained models, namely BERT (<cite class="ltx_cite ltx_citemacro_cite">Jacob
                                Devin and Toutanova (<a href="#bib.bib16"
                                                        title="BERT: pre-training of deep bidirectional transformers for language understanding"
                                                        class="ltx_ref">2019</a>)</cite>), RoBERTa (<cite
                                    class="ltx_cite ltx_citemacro_cite">Liu and Stoyanov (<a href="#bib.bib17"
                                                                                             title="RoBERTa: a robustly optimized bert pretraining approach"
                                                                                             class="ltx_ref">2019</a>)</cite>),
                            XLNet (<cite class="ltx_cite ltx_citemacro_cite">Yang and V. Le (<a href="#bib.bib18"
                                                                                                title="XLNet: generalized autoregressive pretraining for language understanding"
                                                                                                class="ltx_ref">2019</a>)</cite>)
                            on the extracted corpus and calculated the semantic similarity between the meme caption and
                            audio label embeddings. This semantic textual similarity is measured via cosine-similarity
                            as illustrated in Equation <a href="#S6.E3"
                                                          title="(3) ‣ 6.4 Audio Mapper ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                          class="ltx_ref"><span
                                    class="ltx_text ltx_ref_tag">3</span></a>.</p>
                        <table id="S6.E3" class="ltx_equation ltx_eqn_table">

                            <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
                                <td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
                                <td class="ltx_eqn_cell ltx_align_center">
                                    <math id="S6.E3.m1" class="ltx_Math" alttext="similarity=\cos{\theta}=\frac{A\cdot B}{\lvert A\rvert\cdot\lvert B\rvert}=%
\frac{\sum_{i=1}^{n}{A_{i}}{B_{i}}}{\sqrt{\sum_{i=1}^{n}{A_{i}^{2}}}\sqrt{\sum%
_{i=1}^{n}{B_{i}^{2}}}}" display="block">
                                        <mrow>
                                            <mrow>
                                                <mi>s</mi>
                                                <mo>⁢</mo>
                                                <mi>i</mi>
                                                <mo>⁢</mo>
                                                <mi>m</mi>
                                                <mo>⁢</mo>
                                                <mi>i</mi>
                                                <mo>⁢</mo>
                                                <mi>l</mi>
                                                <mo>⁢</mo>
                                                <mi>a</mi>
                                                <mo>⁢</mo>
                                                <mi>r</mi>
                                                <mo>⁢</mo>
                                                <mi>i</mi>
                                                <mo>⁢</mo>
                                                <mi>t</mi>
                                                <mo>⁢</mo>
                                                <mi>y</mi>
                                            </mrow>
                                            <mo>=</mo>
                                            <mrow>
                                                <mi>cos</mi>
                                                <mo>⁡</mo>
                                                <mi>θ</mi>
                                            </mrow>
                                            <mo>=</mo>
                                            <mfrac>
                                                <mrow>
                                                    <mi>A</mi>
                                                    <mo>⋅</mo>
                                                    <mi>B</mi>
                                                </mrow>
                                                <mrow>
                                                    <mrow>
                                                        <mo fence="true" stretchy="false">|</mo>
                                                        <mi>A</mi>
                                                        <mo fence="true" stretchy="false">|</mo>
                                                    </mrow>
                                                    <mo>⋅</mo>
                                                    <mrow>
                                                        <mo fence="true" stretchy="false">|</mo>
                                                        <mi>B</mi>
                                                        <mo fence="true" stretchy="false">|</mo>
                                                    </mrow>
                                                </mrow>
                                            </mfrac>
                                            <mo>=</mo>
                                            <mfrac>
                                                <mrow>
                                                    <msubsup>
                                                        <mo largeop="true" symmetric="true">∑</mo>
                                                        <mrow>
                                                            <mi>i</mi>
                                                            <mo>=</mo>
                                                            <mn>1</mn>
                                                        </mrow>
                                                        <mi>n</mi>
                                                    </msubsup>
                                                    <mrow>
                                                        <msub>
                                                            <mi>A</mi>
                                                            <mi>i</mi>
                                                        </msub>
                                                        <mo>⁢</mo>
                                                        <msub>
                                                            <mi>B</mi>
                                                            <mi>i</mi>
                                                        </msub>
                                                    </mrow>
                                                </mrow>
                                                <mrow>
                                                    <msqrt>
                                                        <mrow>
                                                            <msubsup>
                                                                <mo largeop="true" symmetric="true">∑</mo>
                                                                <mrow>
                                                                    <mi>i</mi>
                                                                    <mo>=</mo>
                                                                    <mn>1</mn>
                                                                </mrow>
                                                                <mi>n</mi>
                                                            </msubsup>
                                                            <msubsup>
                                                                <mi>A</mi>
                                                                <mi>i</mi>
                                                                <mn>2</mn>
                                                            </msubsup>
                                                        </mrow>
                                                    </msqrt>
                                                    <mo>⁢</mo>
                                                    <msqrt>
                                                        <mrow>
                                                            <msubsup>
                                                                <mo largeop="true" symmetric="true">∑</mo>
                                                                <mrow>
                                                                    <mi>i</mi>
                                                                    <mo>=</mo>
                                                                    <mn>1</mn>
                                                                </mrow>
                                                                <mi>n</mi>
                                                            </msubsup>
                                                            <msubsup>
                                                                <mi>B</mi>
                                                                <mi>i</mi>
                                                                <mn>2</mn>
                                                            </msubsup>
                                                        </mrow>
                                                    </msqrt>
                                                </mrow>
                                            </mfrac>
                                        </mrow>
                                    </math>
                                </td>
                                <td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
                                <td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span
                                        class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
                            </tr>
                        </table>
                    </div>
                </section>
            </section>
            <section id="S7" class="ltx_section">
                <h2 class="ltx_title ltx_title_section">
                    <span class="ltx_tag ltx_tag_section">7 </span>Experiments and Evaluation</h2>

                <section id="S7.SS1" class="ltx_subsection">
                    <h3 class="ltx_title ltx_title_subsection">
                        <span class="ltx_tag ltx_tag_subsection">7.1 </span>Image Caption Generation</h3>

                    <div id="S7.SS1.p1" class="ltx_para">
                        <p class="ltx_p">To evaluate the model’s performance on the validation set (split into the
                            subsets defined by <cite class="ltx_cite ltx_citemacro_cite">Karpathy and Fei-Fei (<a
                                    href="#bib.bib12"
                                    title="Deep visual-semantic alignments for generating image descriptions"
                                    class="ltx_ref">2014</a>)</cite>), we will use the automated BiLingual Evaluation
                            Understudy (BLEU) evaluation metric as formulated in Equation <a href="#S7.E4"
                                                                                             title="(4) ‣ 7.1 Image Caption Generation ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
                                                                                             class="ltx_ref"><span
                                    class="ltx_text ltx_ref_tag">4</span></a> (<cite
                                    class="ltx_cite ltx_citemacro_cite">Kishore Papineni and Zhu (<a href="#bib.bib13"
                                                                                                     title="BLEU: a method for automatic evaluation of machine translation"
                                                                                                     class="ltx_ref">2001</a>)</cite>).
                            The metric evaluates a generated caption against reference caption(s).</p>
                    </div>
                    <div id="S7.SS1.p2" class="ltx_para">
                        <table id="S7.E4" class="ltx_equation ltx_eqn_table">

                            <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
                                <td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
                                <td class="ltx_eqn_cell ltx_align_center">
                                    <math id="S7.E4.m1" class="ltx_Math"
                                          alttext="BLEU=BP\cdot e^{\sum_{n=1}^{N}(w_{n}\log{p_{n}})}" display="block">
                                        <mrow>
                                            <mrow>
                                                <mi>B</mi>
                                                <mo>⁢</mo>
                                                <mi>L</mi>
                                                <mo>⁢</mo>
                                                <mi>E</mi>
                                                <mo>⁢</mo>
                                                <mi>U</mi>
                                            </mrow>
                                            <mo>=</mo>
                                            <mrow>
                                                <mrow>
                                                    <mi>B</mi>
                                                    <mo>⁢</mo>
                                                    <mi>P</mi>
                                                </mrow>
                                                <mo>⋅</mo>
                                                <msup>
                                                    <mi>e</mi>
                                                    <mrow>
                                                        <mstyle displaystyle="false">
                                                            <msubsup>
                                                                <mo largeop="true" symmetric="true">∑</mo>
                                                                <mrow>
                                                                    <mi>n</mi>
                                                                    <mo>=</mo>
                                                                    <mn>1</mn>
                                                                </mrow>
                                                                <mi>N</mi>
                                                            </msubsup>
                                                        </mstyle>
                                                        <mrow>
                                                            <mo stretchy="false">(</mo>
                                                            <mrow>
                                                                <msub>
                                                                    <mi>w</mi>
                                                                    <mi>n</mi>
                                                                </msub>
                                                                <mo>⁢</mo>
                                                                <mrow>
                                                                    <mi>log</mi>
                                                                    <mo>⁡</mo>
                                                                    <msub>
                                                                        <mi>p</mi>
                                                                        <mi>n</mi>
                                                                    </msub>
                                                                </mrow>
                                                            </mrow>
                                                            <mo stretchy="false">)</mo>
                                                        </mrow>
                                                    </mrow>
                                                </msup>
                                            </mrow>
                                        </mrow>
                                    </math>
                                </td>
                                <td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
                                <td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span
                                        class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
                            </tr>
                        </table>
                        <p class="ltx_p">where
                            <math id="S7.SS1.p2.m1" class="ltx_Math" alttext="p_{n}" display="inline">
                                <msub>
                                    <mi>p</mi>
                                    <mi>n</mi>
                                </msub>
                            </math>
                            is the modified precision for ngram, the base of log is the natural base e,
                            <math id="S7.SS1.p2.m2" class="ltx_Math" alttext="w_{n}" display="inline">
                                <msub>
                                    <mi>w</mi>
                                    <mi>n</mi>
                                </msub>
                            </math>
                            is weight between 0 and 1 for
                            <math id="S7.SS1.p2.m3" class="ltx_Math" alttext="\log{p_{n}}" display="inline">
                                <mrow>
                                    <mi>log</mi>
                                    <mo>⁡</mo>
                                    <msub>
                                        <mi>p</mi>
                                        <mi>n</mi>
                                    </msub>
                                </mrow>
                            </math>
                            and
                            <math id="S7.SS1.p2.m4" class="ltx_Math" alttext="\sum_{n=1}^{N}w_{n}=1" display="inline">
                                <mrow>
                                    <mrow>
                                        <msubsup>
                                            <mo largeop="true" symmetric="true">∑</mo>
                                            <mrow>
                                                <mi>n</mi>
                                                <mo>=</mo>
                                                <mn>1</mn>
                                            </mrow>
                                            <mi>N</mi>
                                        </msubsup>
                                        <msub>
                                            <mi>w</mi>
                                            <mi>n</mi>
                                        </msub>
                                    </mrow>
                                    <mo>=</mo>
                                    <mn>1</mn>
                                </mrow>
                            </math>
                            , and BP as formulated in Equation <a href="#S7.E5"
                                                                  title="(5) ‣ 7.1 Image Caption Generation ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
                                                                  class="ltx_ref"><span
                                    class="ltx_text ltx_ref_tag">5</span></a>) is the brevity penalty to penalize short
                            machine translations.
                        </p>
                    </div>
                    <div id="S7.SS1.p3" class="ltx_para">
                        <table id="S7.E5" class="ltx_equation ltx_eqn_table">

                            <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
                                <td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
                                <td class="ltx_eqn_cell ltx_align_center">
                                    <math id="S7.E5.m1" class="ltx_Math" alttext="BP=\begin{cases}1&amp;\text{if }c&gt;r\\
\exp{(1-\frac{r}{c})}&amp;\text{if }c\leq r\end{cases}" display="block">
                                        <mrow>
                                            <mrow>
                                                <mi>B</mi>
                                                <mo>⁢</mo>
                                                <mi>P</mi>
                                            </mrow>
                                            <mo>=</mo>
                                            <mrow>
                                                <mo>{</mo>
                                                <mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt">
                                                    <mtr>
                                                        <mtd columnalign="left">
                                                            <mn>1</mn>
                                                        </mtd>
                                                        <mtd columnalign="left">
                                                            <mrow>
                                                                <mrow>
                                                                    <mtext>if </mtext>
                                                                    <mo>⁢</mo>
                                                                    <mi>c</mi>
                                                                </mrow>
                                                                <mo>&gt;</mo>
                                                                <mi>r</mi>
                                                            </mrow>
                                                        </mtd>
                                                    </mtr>
                                                    <mtr>
                                                        <mtd columnalign="left">
                                                            <mrow>
                                                                <mi>exp</mi>
                                                                <mo>⁡</mo>
                                                                <mrow>
                                                                    <mo stretchy="false">(</mo>
                                                                    <mrow>
                                                                        <mn>1</mn>
                                                                        <mo>-</mo>
                                                                        <mstyle displaystyle="false">
                                                                            <mfrac>
                                                                                <mi>r</mi>
                                                                                <mi>c</mi>
                                                                            </mfrac>
                                                                        </mstyle>
                                                                    </mrow>
                                                                    <mo stretchy="false">)</mo>
                                                                </mrow>
                                                            </mrow>
                                                        </mtd>
                                                        <mtd columnalign="left">
                                                            <mrow>
                                                                <mrow>
                                                                    <mtext>if </mtext>
                                                                    <mo>⁢</mo>
                                                                    <mi>c</mi>
                                                                </mrow>
                                                                <mo>≤</mo>
                                                                <mi>r</mi>
                                                            </mrow>
                                                        </mtd>
                                                    </mtr>
                                                </mtable>
                                            </mrow>
                                        </mrow>
                                    </math>
                                </td>
                                <td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
                                <td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span
                                        class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
                            </tr>
                        </table>
                    </div>
                    <div id="S7.SS1.p4" class="ltx_para">
                        <p class="ltx_p">where
                            <math id="S7.SS1.p4.m1" class="ltx_Math" alttext="c" display="inline">
                                <mi>c</mi>
                            </math>
                            is the number of unigrams (length) in all the candidate sentences, and
                            <math id="S7.SS1.p4.m2" class="ltx_Math" alttext="r" display="inline">
                                <mi>r</mi>
                            </math>
                            is the best match lengths for each candidate sentence in the corpus.
                        </p>
                    </div>
                    <div id="S7.SS1.p5" class="ltx_para">
                        <p class="ltx_p">BLEU-4 score of the model on MSCOCO (<cite class="ltx_cite ltx_citemacro_cite">Lin
                            (<a href="#bib.bib14" title="Microsoft coco: common objects in context." class="ltx_ref">2014</a>)</cite>)
                            dataset is shown in Table <a href="#S7.T1"
                                                         title="Table 1 ‣ 7.1 Image Caption Generation ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
                                                         class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
                        </p>
                    </div>
                    <figure id="S7.T1" class="ltx_table">
                        <table class="ltx_tabular ltx_centering ltx_align_middle">
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Beam Size
                                </td>
                                <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Validation BLEU-4</td>
                                <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Test BLEU-4</td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">1</td>
                                <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.98</td>
                                <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.28</td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r">2</td>
                                <td class="ltx_td ltx_align_center ltx_border_r">32.95</td>
                                <td class="ltx_td ltx_align_center ltx_border_r">33.06</td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span
                                        class="ltx_text ltx_font_bold">5</span></td>
                                <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span
                                        class="ltx_text ltx_font_bold">33.17</span></td>
                                <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span
                                        class="ltx_text ltx_font_bold">33.29</span></td>
                            </tr>
                        </table>
                        <figcaption class="ltx_caption ltx_centering"><span
                                class="ltx_tag ltx_tag_table">Table 1: </span>BLEU-4 scores indicating the model’s
                            performance for different beam sizes
                        </figcaption>
                    </figure>
                    <div id="S7.SS1.p6" class="ltx_para">
                        <p class="ltx_p">Some examples of captions generated by the model and the visualization of the
                            attention component for each word in the caption is demonstrated in Fig <a href="#S7.F8"
                                                                                                       title="Figure 8 ‣ 7.1 Image Caption Generation ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
                                                                                                       class="ltx_ref"><span
                                    class="ltx_text ltx_ref_tag">8</span></a>.</p>
                    </div>
                    <figure id="S7.F8" class="ltx_figure"><img src="exampleofimagecaption.png" id="S7.F8.g1"
                                                               class="ltx_graphics ltx_centering" width="284"
                                                               height="291" alt="">
                        <figcaption class="ltx_caption ltx_centering"><span
                                class="ltx_tag ltx_tag_figure">Figure 8: </span>Images with its attention at every step
                            of word generation and generated caption
                        </figcaption>
                    </figure>
                    <figure id="S7.T2" class="ltx_table">
                        <table class="ltx_tabular ltx_centering ltx_align_middle">
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_border_tt"></td>
                                <td class="ltx_td ltx_align_center ltx_border_tt" colspan="2">test dataset - Accuracy
                                </td>
                                <td class="ltx_td ltx_align_center ltx_border_tt" colspan="2">test dataset - Loss</td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center"># of templates</td>
                                <td class="ltx_td ltx_align_center ltx_border_t">LSTM</td>
                                <td class="ltx_td ltx_align_center ltx_border_t">BERT</td>
                                <td class="ltx_td ltx_align_center ltx_border_t">LSTM</td>
                                <td class="ltx_td ltx_align_center ltx_border_t">BERT</td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_t"><span
                                        class="ltx_text ltx_font_bold">20</span></td>
                                <td class="ltx_td ltx_align_center ltx_border_t">
                                    <span class="ltx_text ltx_font_bold">52</span>%
                                </td>
                                <td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">65%</span>
                                </td>
                                <td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">1.80</span>
                                </td>
                                <td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">1.32</span>
                                </td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center">50</td>
                                <td class="ltx_td ltx_align_center">51%</td>
                                <td class="ltx_td ltx_align_center">63%</td>
                                <td class="ltx_td ltx_align_center">2.38</td>
                                <td class="ltx_td ltx_align_center">1.48</td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center">90</td>
                                <td class="ltx_td ltx_align_center">–</td>
                                <td class="ltx_td ltx_align_center">55%</td>
                                <td class="ltx_td ltx_align_center">–</td>
                                <td class="ltx_td ltx_align_center">1.88</td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center">300</td>
                                <td class="ltx_td ltx_align_center">–</td>
                                <td class="ltx_td ltx_align_center">40%</td>
                                <td class="ltx_td ltx_align_center">–</td>
                                <td class="ltx_td ltx_align_center">1.89</td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_tt"></td>
                                <td class="ltx_td ltx_border_tt"></td>
                                <td class="ltx_td ltx_border_tt"></td>
                                <td class="ltx_td ltx_border_tt"></td>
                                <td class="ltx_td ltx_border_tt"></td>
                            </tr>
                        </table>
                        <figcaption class="ltx_caption ltx_centering"><span
                                class="ltx_tag ltx_tag_table">Table 2: </span>Meme template selection module performance
                            on the test dataset using BERT and LSTM.
                        </figcaption>
                    </figure>
                </section>
                <section id="S7.SS2" class="ltx_subsection">
                    <h3 class="ltx_title ltx_title_subsection">
                        <span class="ltx_tag ltx_tag_subsection">7.2 </span>Meme template selection</h3>

                    <div id="S7.SS2.p1" class="ltx_para">
                        <p class="ltx_p">After fine tuning different pre-trained language representation models (BERT,
                            XLNet, and Roberta) with a single neural network, we found BERT to outperform other models.
                            We have also experimented with word2vec embeddings for the meme template selection task.Our
                            dataset has 3000 captions for a template and split of 80,10,10 for training, validation and
                            testing respectively.
                            <br class="ltx_break">We trained the model for different number of templates to compare the
                            results as shown in the Table <a href="#S7.T2"
                                                             title="Table 2 ‣ 7.1 Image Caption Generation ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
                                                             class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
                            Since commonly used meme templates on social media are less in number and a single meme
                            template can convey multiple meanings, we have decided to use a model trained with 50
                            templates.</p>
                    </div>
                </section>
                <section id="S7.SS3" class="ltx_subsection">
                    <h3 class="ltx_title ltx_title_subsection">
                        <span class="ltx_tag ltx_tag_subsection">7.3 </span>Meme caption generation</h3>

                    <div id="S7.SS3.p1" class="ltx_para">
                        <p class="ltx_p">Experiments were conducted with and without labels as input to the LSTMs
                            employed for generating meme captions. In order to evaluate the meme captions generated two
                            metrics were utilized, the perplexity, and the hilarity as demonstrated in Table <a
                                    href="#S7.T3"
                                    title="Table 3 ‣ 7.3 Meme caption generation ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
                                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The Perplexity (PP)
                            is a measure of the inverse probabilities of predicting the next word in the example caption
                            (C) formulated as Equation <a href="#S7.E6"
                                                          title="(6) ‣ 7.3 Meme caption generation ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
                                                          class="ltx_ref"><span
                                    class="ltx_text ltx_ref_tag">6</span></a>.
                        </p>
                        <table id="S7.E6" class="ltx_equation ltx_eqn_table">

                            <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
                                <td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
                                <td class="ltx_eqn_cell ltx_align_center">
                                    <math id="S7.E6.m1" class="ltx_Math"
                                          alttext="PP(C)=\sqrt[N]{\prod_{i=1}^{N}\frac{1}{P(w_{i}|w_{1}\cdot\cdot\cdot w_{i-1})}}"
                                          display="block">
                                        <mrow>
                                            <mrow>
                                                <mi>P</mi>
                                                <mo>⁢</mo>
                                                <mi>P</mi>
                                                <mo>⁢</mo>
                                                <mrow>
                                                    <mo stretchy="false">(</mo>
                                                    <mi>C</mi>
                                                    <mo stretchy="false">)</mo>
                                                </mrow>
                                            </mrow>
                                            <mo>=</mo>
                                            <mroot>
                                                <mrow>
                                                    <munderover>
                                                        <mo largeop="true" movablelimits="false" symmetric="true">∏</mo>
                                                        <mrow>
                                                            <mi>i</mi>
                                                            <mo>=</mo>
                                                            <mn>1</mn>
                                                        </mrow>
                                                        <mi>N</mi>
                                                    </munderover>
                                                    <mfrac>
                                                        <mn>1</mn>
                                                        <mrow>
                                                            <mi>P</mi>
                                                            <mo>⁢</mo>
                                                            <mrow>
                                                                <mo stretchy="false">(</mo>
                                                                <mrow>
                                                                    <msub>
                                                                        <mi>w</mi>
                                                                        <mi>i</mi>
                                                                    </msub>
                                                                    <mo lspace="2.5pt" rspace="2.5pt" stretchy="false">
                                                                        |
                                                                    </mo>
                                                                    <mrow>
                                                                        <msub>
                                                                            <mi>w</mi>
                                                                            <mn>1</mn>
                                                                        </msub>
                                                                        <mo>⁢</mo>
                                                                        <mi mathvariant="normal">⋯</mi>
                                                                        <mo>⁢</mo>
                                                                        <msub>
                                                                            <mi>w</mi>
                                                                            <mrow>
                                                                                <mi>i</mi>
                                                                                <mo>-</mo>
                                                                                <mn>1</mn>
                                                                            </mrow>
                                                                        </msub>
                                                                    </mrow>
                                                                </mrow>
                                                                <mo stretchy="false">)</mo>
                                                            </mrow>
                                                        </mrow>
                                                    </mfrac>
                                                </mrow>
                                                <mi>N</mi>
                                            </mroot>
                                        </mrow>
                                    </math>
                                </td>
                                <td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
                                <td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span
                                        class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
                            </tr>
                        </table>
                        <p class="ltx_p">
                            <math id="S7.SS3.p1.m1" class="ltx_Math" alttext="\{w_{1}...w_{N}\}" display="inline">
                                <mrow>
                                    <mo stretchy="false">{</mo>
                                    <mrow>
                                        <msub>
                                            <mi>w</mi>
                                            <mn>1</mn>
                                        </msub>
                                        <mo>⁢</mo>
                                        <mi mathvariant="normal">…</mi>
                                        <mo>⁢</mo>
                                        <msub>
                                            <mi>w</mi>
                                            <mi>N</mi>
                                        </msub>
                                    </mrow>
                                    <mo stretchy="false">}</mo>
                                </mrow>
                            </math>
                            are the words in caption C.
                            The probabilities P can be found from the cross entropy loss calculated on the LSTM output.
                            Low perplexity means the model frequently assigns high probabilities to the given evaluation
                            example captions. This metric indicates the model’s performance on predicting captions for
                            images of different formats with the correct style. It is a limited metric for success as it
                            tells us nothing about whether the captions are humorous, original and varied.
                            For evaluating hilarity, we five team members have rated the level of correctness and
                            hilarity on a scale of 1 to 5 and averaged it out. We have sent out survey forms to other
                            students.
                        </p>
                    </div>
                    <figure id="S7.T3" class="ltx_table">
                        <table class="ltx_tabular ltx_centering ltx_align_middle">
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Model</td>
                                <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Perplexity</td>
                                <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Hilarity</td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Model without
                                    meme template label names
                                </td>
                                <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.42</td>
                                <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.24</td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span
                                        class="ltx_text ltx_font_bold">Model with meme template label names</span></td>
                                <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span
                                        class="ltx_text ltx_font_bold">2.25</span></td>
                                <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span
                                        class="ltx_text ltx_font_bold">3.73</span></td>
                            </tr>
                        </table>
                        <figcaption class="ltx_caption ltx_centering"><span
                                class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of performance of the meme
                            generation module with and without labels.
                        </figcaption>
                    </figure>
                </section>
                <section id="S7.SS4" class="ltx_subsection">
                    <h3 class="ltx_title ltx_title_subsection">
                        <span class="ltx_tag ltx_tag_subsection">7.4 </span>Audio Mapper</h3>

                    <div id="S7.SS4.p1" class="ltx_para">
                        <p class="ltx_p">Here we performed performance comparison analysis with multiple pre-trained
                            transformer based models, namely BERT (<cite class="ltx_cite ltx_citemacro_cite">Jacob Devin
                                and Toutanova (<a href="#bib.bib16"
                                                  title="BERT: pre-training of deep bidirectional transformers for language understanding"
                                                  class="ltx_ref">2019</a>)</cite>), RoBERTa (<cite
                                    class="ltx_cite ltx_citemacro_cite">Liu and Stoyanov (<a href="#bib.bib17"
                                                                                             title="RoBERTa: a robustly optimized bert pretraining approach"
                                                                                             class="ltx_ref">2019</a>)</cite>),
                            ALBERT (<cite class="ltx_cite ltx_citemacro_cite">Lan and Soricut (<a href="#bib.bib19"
                                                                                                  title="ALBERT: a lite bert for self-supervised learning of language representations"
                                                                                                  class="ltx_ref">2019</a>)</cite>)
                            and XLNet(<cite class="ltx_cite ltx_citemacro_cite">Yang and V. Le (<a href="#bib.bib18"
                                                                                                   title="XLNet: generalized autoregressive pretraining for language understanding"
                                                                                                   class="ltx_ref">2019</a>)</cite>)
                            to generate accurate similarity scores between the generated meme caption and the funny
                            audio label dataset. We observed that a fine-tuned RoBERTa produces the most accurate
                            mappings between the meme and our extracted audio corpus. Training and fine tuning of the
                            network is done with a siamese or triplet network structure.</p>
                    </div>
                    <figure id="S7.T4" class="ltx_table">
                        <table class="ltx_tabular ltx_centering ltx_align_middle">
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">Input Image</span>
                                </td>
                                <td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">Caption</span>
                                </td>
                                <td class="ltx_td ltx_align_right">
                                    <span class="ltx_ERROR undefined">\makecell</span><span
                                        class="ltx_text ltx_font_bold">Meme Template</span>
                                </td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">Label</span>
                                </td>
                                <td class="ltx_td ltx_align_right">
                                    <span class="ltx_ERROR undefined">\makecell</span><span
                                        class="ltx_text ltx_font_bold">Meme Clip</span>
                                </td>
                                <td class="ltx_td"></td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">Link</span>
                                </td>
                                <td class="ltx_td"></td>
                                <td class="ltx_td"></td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
                                    <span class="ltx_ERROR undefined">\makecell</span><img src="1.jpeg" id="S7.T4.g1"
                                                                                           class="ltx_graphics"
                                                                                           width="132" height="88"
                                                                                           alt="">
                                </td>
                                <td class="ltx_td ltx_align_right ltx_border_tt">
                                    <span class="ltx_ERROR undefined">\makecell</span>a man riding
                                </td>
                                <td class="ltx_td ltx_border_tt"></td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r">a wave on top</td>
                                <td class="ltx_td"></td>
                                <td class="ltx_td"></td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r">of a surfboard</td>
                                <td class="ltx_td ltx_align_center ltx_border_r">Stoner Stanley</td>
                                <td class="ltx_td ltx_align_center ltx_border_r"><a href="https://tinyurl.com/y4bccg8p"
                                                                                    title="" class="ltx_ref ltx_href">Click
                                    Me</a></td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
                                    <span class="ltx_ERROR undefined">\makecell</span><img src="cat.jpg" id="S7.T4.g2"
                                                                                           class="ltx_graphics"
                                                                                           width="42" height="28"
                                                                                           alt="">
                                </td>
                                <td class="ltx_td ltx_align_right ltx_border_tt">
                                    <span class="ltx_ERROR undefined">\makecell</span>a black
                                </td>
                                <td class="ltx_td ltx_border_tt"></td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r">and white cat sitting</td>
                                <td class="ltx_td"></td>
                                <td class="ltx_td"></td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r">on a window</td>
                                <td class="ltx_td"></td>
                                <td class="ltx_td"></td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r">sill</td>
                                <td class="ltx_td ltx_align_center ltx_border_r">Awkward Seal</td>
                                <td class="ltx_td ltx_align_center ltx_border_r"><a href="https://tinyurl.com/y68wz4yj"
                                                                                    title="" class="ltx_ref ltx_href">Click
                                    Me</a></td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
                                    <span class="ltx_ERROR undefined">\makecell</span><img
                                        src="random-shocks-newweb.jpg" id="S7.T4.g3" class="ltx_graphics" width="150"
                                        height="75" alt="">
                                </td>
                                <td class="ltx_td ltx_align_right ltx_border_tt">
                                    <span class="ltx_ERROR undefined">\makecell</span>a close up
                                </td>
                                <td class="ltx_td ltx_border_tt"></td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r">of a bird</td>
                                <td class="ltx_td"></td>
                                <td class="ltx_td"></td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r">in the water</td>
                                <td class="ltx_td ltx_align_right">
                                    <span class="ltx_ERROR undefined">\makecell</span>you don’t
                                </td>
                                <td class="ltx_td"></td>
                            </tr>
                            <tr class="ltx_tr">
                                <td class="ltx_td ltx_align_center ltx_border_r">say meme</td>
                                <td class="ltx_td ltx_align_center ltx_border_r"><a href="https://tinyurl.com/y6zuu5gf"
                                                                                    title="" class="ltx_ref ltx_href">Click
                                    Me</a></td>
                                <td class="ltx_td"></td>
                            </tr>
                        </table>
                        <figcaption class="ltx_caption ltx_centering"><span
                                class="ltx_tag ltx_tag_table">Table 4: </span>Output of different modules of ClipMe
                        </figcaption>
                    </figure>
                </section>
            </section>
            <section id="S8" class="ltx_section">
                <h2 class="ltx_title ltx_title_section">
                    <span class="ltx_tag ltx_tag_section">8 </span>Results</h2>

                <div id="S8.p1" class="ltx_para">
                    <p class="ltx_p">As depicted in Fig <a href="#S4.F1"
                                                           title="Figure 1 ‣ 4 Problem Formulation ‣ ClipMe: Meme Clip Generation"
                                                           class="ltx_ref"><span
                            class="ltx_text ltx_ref_tag">1</span></a>, the model first generates a meaningful caption
                        for the given input image. This caption is fed into the meme template selection module that
                        selects the best match meme template for the input caption. The selected meme template and the
                        image caption then acts as an input for Meme Caption Generator that generates a funny meme
                        caption. The funny meme caption is embedded in the meme template to create the final meme
                        embedded with a humorous audio matching the meme caption.
                        Table <a href="#S7.T4"
                                 title="Table 4 ‣ 7.4 Audio Mapper ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
                                 class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows a few
                        demonstrations of the output of the ClipMe architecture.</p>
                </div>
            </section>
            <section id="S9" class="ltx_section">
                <h2 class="ltx_title ltx_title_section">
                    <span class="ltx_tag ltx_tag_section">9 </span>Remaining steps</h2>

                <div id="S9.p1" class="ltx_para">
                    <p class="ltx_p">We have developed and baselined all the individual modules. To enhance the
                        performance of our novel architecture, we decided to create a new custom dataset spanning the
                        entire project pipeline and fine-tune all modules on that data. The future tasks are as
                        follows:</p>
                    <ul id="S9.I1" class="ltx_itemize">
                        <li id="S9.I1.i1" class="ltx_item" style="list-style-type:none;">
                            <span class="ltx_tag ltx_tag_item">•</span>
                            <div id="S9.I1.i1.p1" class="ltx_para">
                                <p class="ltx_p">Create a custom hand engineered dataset spanning the entire solution
                                    pipeline, i.e. for each input image data will contain Image-caption mappings,
                                    caption-template mappings, template+caption to meme mappings and meme to audio
                                    mappings.</p>
                            </div>
                        </li>
                        <li id="S9.I1.i2" class="ltx_item" style="list-style-type:none;">
                            <span class="ltx_tag ltx_tag_item">•</span>
                            <div id="S9.I1.i2.p1" class="ltx_para">
                                <p class="ltx_p">Fine tune the networks on the new dataset.</p>
                            </div>
                        </li>
                        <li id="S9.I1.i3" class="ltx_item" style="list-style-type:none;">
                            <span class="ltx_tag ltx_tag_item">•</span>
                            <div id="S9.I1.i3.p1" class="ltx_para">
                                <p class="ltx_p">Experiment with different network hyperparameters and architectures to
                                    optimize performance.</p>
                            </div>
                        </li>
                        <li id="S9.I1.i4" class="ltx_item" style="list-style-type:none;">
                            <span class="ltx_tag ltx_tag_item">•</span>
                            <div id="S9.I1.i4.p1" class="ltx_para">
                                <p class="ltx_p">Explore new evaluation metrics to better benchmark our model
                                    performance</p>
                            </div>
                        </li>
                    </ul>
                </div>
            </section>
            <section id="S10" class="ltx_section">
                <h2 class="ltx_title ltx_title_section">
                    <span class="ltx_tag ltx_tag_section">10 </span>Breakdown of individual contributions</h2>

                <div id="S10.p1" class="ltx_para">
                    <ul id="S10.I1" class="ltx_itemize">
                        <li id="S10.I1.i1" class="ltx_item" style="list-style-type:none;">
                            <span class="ltx_tag ltx_tag_item">•</span>
                            <div id="S10.I1.i1.p1" class="ltx_para">
                                <p class="ltx_p">Mehak Piplani</p>
                                <ul id="S10.I1.I1" class="ltx_itemize">
                                    <li id="S10.I1.i1.i1" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i1.i1.p1" class="ltx_para">
                                            <p class="ltx_p">Brainstorming and coming up with the idea of doing a
                                                project based on automated meme generation.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i1.i2" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i1.i2.p1" class="ltx_para">
                                            <p class="ltx_p">Brainstorming and bringing novelty to the project by the
                                                concept of meme clip by incorporating audio to the meme.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i1.i3" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i1.i3.p1" class="ltx_para">
                                            <p class="ltx_p">Implementation of the Meme Generation Model
                                                (encoder-decoder model with image captions and meme template embedding
                                                and meme template label as input) and fine-tuning the model improving
                                                the perplexity from 2.54 to 2.25.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i1.i4" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i1.i4.p1" class="ltx_para">
                                            <p class="ltx_p">Integration of the image caption generation module, the
                                                audio mapper, and the meme generation module for inference and creation
                                                of the custom dataset.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i1.i5" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i1.i5.p1" class="ltx_para">
                                            <p class="ltx_p">Implementation and Fine-tuning meme template selection
                                                using models: RoBERTa and XLNet.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i1.i6" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i1.i6.p1" class="ltx_para">
                                            <p class="ltx_p">Brainstorming the idea of hilarity metric as the evaluation
                                                metric of meme generation module and meme clip.</p>
                                        </div>
                                    </li>
                                </ul>
                            </div>
                        </li>
                        <li id="S10.I1.i2" class="ltx_item" style="list-style-type:none;">
                            <span class="ltx_tag ltx_tag_item">•</span>
                            <div id="S10.I1.i2.p1" class="ltx_para">
                                <p class="ltx_p">Shivam</p>
                                <ul id="S10.I1.I2" class="ltx_itemize">
                                    <li id="S10.I1.i2.i1" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i2.i1.p1" class="ltx_para">
                                            <p class="ltx_p">Implementation of the Meme Generation Model
                                                (encoder-decoder model with image captions and meme template embedding
                                                as input) and fine-tuning the model improving the perplexity from 2.65
                                                to 2.42.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i2.i2" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i2.i2.p1" class="ltx_para">
                                            <p class="ltx_p">Integration of Meme Template Module and Meme Generation
                                                Module for inference and creation of the custom dataset.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i2.i3" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i2.i3.p1" class="ltx_para">
                                            <p class="ltx_p">Brainstorming and coming up with the idea of audio matching
                                                module.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i2.i4" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i2.i4.p1" class="ltx_para">
                                            <p class="ltx_p">Brainstorming and implementation of Perplexity as the
                                                evaluation metric for meme generation module.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i2.i5" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i2.i5.p1" class="ltx_para">
                                            <p class="ltx_p">Implementation of meme clip module which integrates meme
                                                and relevant audio.</p>
                                        </div>
                                    </li>
                                </ul>
                            </div>
                        </li>
                        <li id="S10.I1.i3" class="ltx_item" style="list-style-type:none;">
                            <span class="ltx_tag ltx_tag_item">•</span>
                            <div id="S10.I1.i3.p1" class="ltx_para">
                                <p class="ltx_p">Rishabh Bansal</p>
                                <ul id="S10.I1.I3" class="ltx_itemize">
                                    <li id="S10.I1.i3.i1" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i3.i1.p1" class="ltx_para">
                                            <p class="ltx_p">Developed and fine-tuned image to caption module</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i3.i2" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i3.i2.p1" class="ltx_para">
                                            <p class="ltx_p">Developed audio mapper module and fine-tuned model on
                                                extracted audio label corpus.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i3.i3" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i3.i3.p1" class="ltx_para">
                                            <p class="ltx_p">Ideated &amp; formulated the novel idea to generate
                                                template memes from visual inputs like images
                                            </p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i3.i4" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i3.i4.p1" class="ltx_para">
                                            <p class="ltx_p">Evaluation metric for image caption module.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i3.i5" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i3.i5.p1" class="ltx_para">
                                            <p class="ltx_p">Model benchmarking and selection in image to caption and
                                                audio mapper module</p>
                                        </div>
                                    </li>
                                </ul>
                            </div>
                        </li>
                        <li id="S10.I1.i4" class="ltx_item" style="list-style-type:none;">
                            <span class="ltx_tag ltx_tag_item">•</span>
                            <div id="S10.I1.i4.p1" class="ltx_para">
                                <p class="ltx_p">Che-Pai Kung</p>
                                <ul id="S10.I1.I4" class="ltx_itemize">
                                    <li id="S10.I1.i4.i1" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i4.i1.p1" class="ltx_para">
                                            <p class="ltx_p">Implementation and fine tuning Meme template selection
                                                module using Bert and LSTM</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i4.i2" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i4.i2.p1" class="ltx_para">
                                            <p class="ltx_p">Meme template crawling and building the dataset</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i4.i3" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i4.i3.p1" class="ltx_para">
                                            <p class="ltx_p">Fine tuning of audio mapping model.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i4.i4" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i4.i4.p1" class="ltx_para">
                                            <p class="ltx_p">Data preparation for image captioning.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i4.i5" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i4.i5.p1" class="ltx_para">
                                            <p class="ltx_p">Creating survey design for hilarity metrix</p>
                                        </div>
                                    </li>
                                </ul>
                            </div>
                        </li>
                        <li id="S10.I1.i5" class="ltx_item" style="list-style-type:none;">
                            <span class="ltx_tag ltx_tag_item">•</span>
                            <div id="S10.I1.i5.p1" class="ltx_para">
                                <p class="ltx_p">Ravinarayana Adkathimar</p>
                                <ul id="S10.I1.I5" class="ltx_itemize">
                                    <li id="S10.I1.i5.i1" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i5.i1.p1" class="ltx_para">
                                            <p class="ltx_p">Explored and evaluated different models like OSCAR and
                                                models with different types of visual attention for Image
                                                captioning.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i5.i2" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i5.i2.p1" class="ltx_para">
                                            <p class="ltx_p">Crawled 2 meme audio websites namely voicy.info and
                                                memelife using selenium and created the dataset of around 1100 audio
                                                clips.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i5.i3" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i5.i3.p1" class="ltx_para">
                                            <p class="ltx_p">Helped in fine tuning of the meme template selection module
                                                with BERT pretrained model to increase the accuracy to 65.7%.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i5.i4" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i5.i4.p1" class="ltx_para">
                                            <p class="ltx_p">Setup project environment on github.</p>
                                        </div>
                                    </li>
                                    <li id="S10.I1.i5.i5" class="ltx_item" style="list-style-type:none;">
                                        <span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
                                        <div id="S10.I1.i5.i5.p1" class="ltx_para">
                                            <p class="ltx_p">Developed inference module for image captioning and meme
                                                template selection module.</p>
                                        </div>
                                    </li>
                                </ul>
                            </div>
                        </li>
                    </ul>
                </div>
            </section>
            <section id="bib" class="ltx_bibliography">
                <h2 class="ltx_title ltx_title_bibliography">References</h2>

                <ul id="bib.L1" class="ltx_biblist">
                    <li id="bib.bib15" class="ltx_bibitem ltx_bib_inproceedings">
                        <span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K.  He  (2015)</span>
                        <span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep residual learning for image recognition.</span>.
</span>
                        <span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">arXiv</span>,
</span>
                        <span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">1512</span>.
</span>
                        <span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.I1.i1.p1"
                                                                              title="1st item ‣ 6.1 Image Caption Generation ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                                              class="ltx_ref"><span
                                class="ltx_text ltx_ref_tag">1st item</span></a>.
</span>
                    </li>
                    <li id="bib.bib12" class="ltx_bibitem ltx_bib_inproceedings">
                        <span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A.  Karpathy  and L.  Fei-Fei  (2014)</span>
                        <span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep visual-semantic alignments for generating image descriptions</span>.
</span>
                        <span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">CoRR</span>,
</span>
                        <span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">abs</span>.
</span>
                        <span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.SS1.p1"
                                                                              title="7.1 Image Caption Generation ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
                                                                              class="ltx_ref"><span
                                class="ltx_text ltx_ref_tag">§7.1</span></a>.
</span>
                    </li>
                    <li id="bib.bib19" class="ltx_bibitem ltx_bib_inproceedings">
                        <span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Z.  Lan  and R.  Soricut  (2019)</span>
                        <span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ALBERT: a lite bert for self-supervised learning of language representations</span>.
</span>
                        <span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ACL</span>,
</span>
                        <span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume"></span>.
</span>
                        <span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.SS4.p1"
                                                                              title="7.4 Audio Mapper ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
                                                                              class="ltx_ref"><span
                                class="ltx_text ltx_ref_tag">§7.4</span></a>.
</span>
                    </li>
                    <li id="bib.bib14" class="ltx_bibitem ltx_bib_inproceedings">
                        <span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T.  Lin  (2014)</span>
                        <span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Microsoft coco: common objects in context.</span>.
</span>
                        <span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IBM Research Report</span>,
</span>
                        <span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">ECCV</span>.
</span>
                        <span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.I1.i1.p1"
                                                                              title="1st item ‣ 6.1 Image Caption Generation ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                                              class="ltx_ref"><span
                                class="ltx_text ltx_ref_tag">1st item</span></a>,
<a href="#S7.SS1.p5" title="7.1 Image Caption Generation ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
   class="ltx_ref"><span class="ltx_text ltx_ref_tag">§7.1</span></a>.
</span>
                    </li>
                    <li id="bib.bib17" class="ltx_bibitem ltx_bib_inproceedings">
                        <span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y.  Liu  and V.  Stoyanov  (2019)</span>
                        <span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">RoBERTa: a robustly optimized bert pretraining approach</span>.
</span>
                        <span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ACL</span>,
</span>
                        <span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume"></span>.
</span>
                        <span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.SS4.p2"
                                                                              title="6.4 Audio Mapper ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                                              class="ltx_ref"><span
                                class="ltx_text ltx_ref_tag">§6.4</span></a>,
<a href="#S7.SS4.p1" title="7.4 Audio Mapper ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
   class="ltx_ref"><span class="ltx_text ltx_ref_tag">§7.4</span></a>.
</span>
                    </li>
                    <li id="bib.bib18" class="ltx_bibitem ltx_bib_inproceedings">
                        <span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Z.  Yang  and Q.  V. Le  (2019)</span>
                        <span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">XLNet: generalized autoregressive pretraining for language understanding</span>.
</span>
                        <span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ACL</span>,
</span>
                        <span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume"></span>.
</span>
                        <span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.SS4.p2"
                                                                              title="6.4 Audio Mapper ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                                              class="ltx_ref"><span
                                class="ltx_text ltx_ref_tag">§6.4</span></a>,
<a href="#S7.SS4.p1" title="7.4 Audio Mapper ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
   class="ltx_ref"><span class="ltx_text ltx_ref_tag">§7.4</span></a>.
</span>
                    </li>
                    <li id="bib.bib9" class="ltx_bibitem ltx_bib_article">
                        <span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">V. AbelL.Peirson and E. Tolunay (2018)</span>
                        <span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dank learning: generating memes using deep neural networks</span>.
</span>
                        <span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">ArXiv</span> <span
                                class="ltx_text ltx_bib_volume">abs/1806.04510</span>.
</span>
                        <span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p1"
                                                                              title="3 Related Work ‣ ClipMe: Meme Clip Generation"
                                                                              class="ltx_ref"><span
                                class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
                    </li>
                    <li id="bib.bib16" class="ltx_bibitem ltx_bib_inproceedings">
                        <span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. L. Jacob  Devin  and K.  Toutanova  (2019)</span>
                        <span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">BERT: pre-training of deep bidirectional transformers for language understanding</span>.
</span>
                        <span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ACL</span>,
</span>
                        <span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">RC22176</span>.
</span>
                        <span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.SS4.p2"
                                                                              title="6.4 Audio Mapper ‣ 6 Our Approach ‣ ClipMe: Meme Clip Generation"
                                                                              class="ltx_ref"><span
                                class="ltx_text ltx_ref_tag">§6.4</span></a>,
<a href="#S7.SS4.p1" title="7.4 Audio Mapper ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
   class="ltx_ref"><span class="ltx_text ltx_ref_tag">§7.4</span></a>.
</span>
                    </li>
                    <li id="bib.bib13" class="ltx_bibitem ltx_bib_inproceedings">
                        <span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. W. Kishore  Papineni  and W.  Zhu  (2001)</span>
                        <span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">BLEU: a method for automatic evaluation of machine translation</span>.
</span>
                        <span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IBM Research Report</span>,
</span>
                        <span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">RC22176</span>.
</span>
                        <span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.SS1.p1"
                                                                              title="7.1 Image Caption Generation ‣ 7 Experiments and Evaluation ‣ ClipMe: Meme Clip Generation"
                                                                              class="ltx_ref"><span
                                class="ltx_text ltx_ref_tag">§7.1</span></a>.
</span>
                    </li>
                    <li id="bib.bib10" class="ltx_bibitem ltx_bib_inproceedings">
                        <span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Luong, H. Pham, and C. D. Manning (2015)</span>
                        <span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Effective approaches to attention-based neural machine translation</span>.
</span>
                        <span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</span>,
</span>
                        <span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Lisbon, Portugal</span>, <span
                                class="ltx_text ltx_bib_pages"> pp. 1412–1421</span>.
</span>
                        <span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a
                                href="https://www.aclweb.org/anthology/D15-1166" title=""
                                class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.18653/v1/D15-1166" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
                        <span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p1"
                                                                              title="3 Related Work ‣ ClipMe: Meme Clip Generation"
                                                                              class="ltx_ref"><span
                                class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
                    </li>
                    <li id="bib.bib11" class="ltx_bibitem ltx_bib_unknown">
                        <span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Sadasivam, K. Gunasekar, H. Davulcu, and Y. Yang (2020)</span>
                        <span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p1"
                                                                              title="3 Related Work ‣ ClipMe: Meme Clip Generation"
                                                                              class="ltx_ref"><span
                                class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
                    </li>
                    <li id="bib.bib8" class="ltx_bibitem ltx_bib_inproceedings">
                        <span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">W. Y. Wang and M. Wen (2015)</span>
                        <span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">I can has cheezburger? a nonparanormal approach to combining textual and visual information for predicting and generating popular meme descriptions</span>.
</span>
                        <span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">HLT-NAACL</span>,
</span>
                        <span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p1"
                                                                              title="3 Related Work ‣ ClipMe: Meme Clip Generation"
                                                                              class="ltx_ref"><span
                                class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
                    </li>
                </ul>
            </section>
        </article>
    </div>
    <footer class="ltx_page_footer">
        <div class="ltx_page">
            ©2020 DL Ninjas
        </div>
    </footer>
</div>
</body>
</html>
